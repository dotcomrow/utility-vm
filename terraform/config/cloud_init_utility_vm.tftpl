#cloud-config
hostname: ${hostname}

# Network configuration for static IP
network:
  version: 2
  ethernets:
    eth0:
      dhcp4: false
      addresses:
        - 10.0.0.200/24
      gateway4: 10.0.0.10
      nameservers:
        addresses:
          - 10.0.0.10
          - 8.8.8.8
        search:
          - internal.lan

users:
  - name: ubuntu
    sudo: ["ALL=(ALL) NOPASSWD:ALL"]
    groups: users, admin
    shell: /bin/bash
    ssh_authorized_keys:
      - ${ssh_keys}

ssh_pwauth: false

package_update: true
package_upgrade: true
packages:
  - qemu-guest-agent
  - apt-transport-https
  - ca-certificates
  - curl
  - gnupg-agent
  - software-properties-common
  - zsh
  - nfs-common
  - gnupg
  - lsb-release
  - nftables
  - linux-firmware
  - alsa-utils
  - ubuntu-drivers-common
  - dnsmasq
  - ethtool
  - jq

growpart:
  mode: auto
  devices: ["/"]

write_files:
  - path: /etc/systemd/system/initial-setup.service
    permissions: '0644'
    content: |
      [Unit]
      Description=Post-bootstrap setup for Kubernetes node
      After=network-online.target
      Wants=network-online.target

      [Service]
      Type=oneshot
      EnvironmentFile=-/etc/environment
      ExecStart=/usr/local/bin/initial-setup.sh
      RemainAfterExit=true

      [Install]
      WantedBy=multi-user.target

  - path: /usr/local/bin/initial-setup.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      # export HTTP_PROXY=http://10.0.0.10:3128
      # export HTTPS_PROXY=http://10.0.0.10:3128
      # export NO_PROXY=127.0.0.1,localhost,10.0.0.0/24,10.96.0.0/12

      CPU_FLAGS="nohz_full=0-59 isolcpus=0-59 rcu_nocbs=0-59"
      sed -i "s|^GRUB_CMDLINE_LINUX=.*|GRUB_CMDLINE_LINUX=\"$CPU_FLAGS\"|" /etc/default/grub
      update-grub
      grub-mkconfig -o /boot/grub/grub.cfg || true

      # update-ca-certificates
      systemctl daemon-reload
      /usr/local/bin/performance-tuning.sh
      sysctl --system
      update-grub
      /usr/local/bin/disable-swap.sh
      /usr/local/bin/setup-docker.sh
      /usr/local/bin/setup-fluentbit.sh
      systemctl enable setup-hosts-dns.service
      systemctl start setup-hosts-dns.service
      /usr/local/bin/setup-utility.sh

      systemctl enable qemu-guest-agent
      systemctl start qemu-guest-agent
      systemctl daemon-reload

  - path: /usr/local/bin/setup-utility.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      echo "üîß Setting up utility VM..."

      # Setup dedicated Harbor disk
      echo "üíæ Setting up dedicated Harbor disk..."
      
      # Format the second disk (100GB) for Harbor storage
      HARBOR_DISK="/dev/sdb"
      HARBOR_MOUNT="/data/harbor"
      
      # Check if disk exists
      if [ -b "$HARBOR_DISK" ]; then
        echo "Found Harbor disk: $HARBOR_DISK"
        
        # Create filesystem if not already formatted
        if ! blkid "$HARBOR_DISK"; then
          echo "Formatting Harbor disk with ext4..."
          mkfs.ext4 -F "$HARBOR_DISK"
        else
          echo "Harbor disk already formatted"
        fi
        
        # Create mount point
        mkdir -p "$HARBOR_MOUNT"
        
        # Get UUID for persistent mounting
        DISK_UUID=$(blkid -s UUID -o value "$HARBOR_DISK")
        
        # Add to fstab if not already present
        if ! grep -q "$DISK_UUID" /etc/fstab; then
          echo "UUID=$DISK_UUID $HARBOR_MOUNT ext4 defaults,noatime 0 2" >> /etc/fstab
          echo "Added Harbor disk to fstab"
        fi
        
        # Mount the disk
        mount -a
        
        # Verify mount
        if mountpoint -q "$HARBOR_MOUNT"; then
          echo "‚úÖ Harbor disk mounted successfully at $HARBOR_MOUNT"
          df -h "$HARBOR_MOUNT"
        else
          echo "‚ùå Failed to mount Harbor disk"
          exit 1
        fi
        
        # Set proper ownership and permissions
        chown -R root:root "$HARBOR_MOUNT"
        chmod 755 "$HARBOR_MOUNT"
        
      else
        echo "‚ö†Ô∏è  Harbor disk $HARBOR_DISK not found, using default storage"
        mkdir -p /data/harbor
      fi

      # Harbor installation and configuration
      
      echo "üì¶ Installing Harbor ${HARBOR_VERSION}..."
      
      # Create harbor directories
      mkdir -p /opt/harbor
      mkdir -p /data/harbor
      
      # Download Harbor
      cd /tmp
      wget -q https://github.com/goharbor/harbor/releases/download/${HARBOR_VERSION}/harbor-offline-installer-${HARBOR_VERSION}.tgz
      tar xvf harbor-offline-installer-${HARBOR_VERSION}.tgz
      cp -r harbor/* /opt/harbor/
      
      # Create Harbor configuration
      cd /opt/harbor
      cp harbor.yml.tmpl harbor.yml
      
      # Configure Harbor with SSL disabled
      cat <<EOF > /opt/harbor/harbor.yml
      # Configuration file of Harbor
      
      # The IP address or hostname to access admin UI and registry service.
      hostname: $(hostname).internal.lan
      
      # http related config
      http:
        # port for http, default is 80. If https enabled, this port will redirect to https port
        port: 80
      
      # https related config - DISABLED for pull-through cache
      # https:
      #   port: 443
      #   certificate:
      #   private_key:
      
      # Uncomment external_url if you want to enable external proxy
      # And when it enabled the hostname will no longer used
      external_url: https://harbor.${RANCHER_DOMAIN}
      
      # The initial password of Harbor admin
      harbor_admin_password: Harbor12345
      
      # Authentication configuration
      # Using database authentication initially - GitHub OAuth will be configured via API
      auth_mode: db_auth
      
      # OIDC authentication settings - DISABLED due to GitHub OAuth compatibility issues
      # GitHub OAuth App integration requires manual configuration through Harbor UI
      # oidc_name: GitHub
      # oidc_endpoint: https://github.com/login/oauth/authorize
      # oidc_client_id: ${HARBOR_OIDC_CLIENT}
      # oidc_client_secret: ${HARBOR_OIDC_SECRET}
      # oidc_scope: read:user,read:org
      # oidc_verify_cert: true
      # oidc_auto_onboard: true
      # oidc_user_claim: login
      # oidc_groups_claim: organizations
      # oidc_admin_group: harbor_admins
      # oidc_group_filter: harbor_users,harbor_admins
      
      # Harbor DB configuration
      database:
        # The password for the root user of Harbor DB
        password: root123
        # The maximum number of connections in the idle connection pool (increased for jobservice)
        max_idle_conns: 100
        # The maximum number of open connections to the database (increased for jobservice)
        max_open_conns: 2000
        # Connection timeout for database operations
        conn_max_lifetime: 5m
        # Connection timeout for database connections
        conn_max_idle_time: 10m
      
      # The default data volume
      data_volume: /data/harbor

      # Harbor Storage settings by default is using /data dir on local filesystem
      # Uncomment storage_service setting If you want to using external storage
      # storage_service:
      #   # ca_bundle is the path to the custom root ca certificate, which will be injected into the truststore
      #   # of registry's and chart repository's containers.  This is usually needed when the user hosts a internal storage with self signed certificate.
      #   ca_bundle:
      
      #   # storage backend, default is filesystem, options include filesystem, azure, gcs, s3, swift and oss
      #   # for more info about this configuration please refer https://docs.docker.com/registry/configuration/
      #   filesystem:
      #     maxthreads: 100
      
      # Trivy configuration
      trivy:
        # ignoreUnfixed The flag to display only fixed vulnerabilities
        ignore_unfixed: false
        # skipUpdate The flag to enable or disable Trivy DB downloads from GitHub
        skip_update: false
        # The offline_scan option prevents Trivy from sending API requests to identify dependencies.
        offline_scan: false
        # insecure The flag to skip verifying registry certificate
        insecure: false
      
      jobservice:
        # Maximum number of job workers in job service (increased for better throughput)
        max_job_workers: 20
        # The timeout for webhook job (increased for large image operations)
        webhook_job_timeout: 300
        # Job processing timeout (5 minutes for large operations)
        job_timeout: 300
        # Pool size for job workers
        pool_size: 50
        # Memory settings for jobservice workers
        worker_memory_mb: 512
        # Maximum queue size for pending jobs
        max_queue_size: 10000
        # Job cleanup settings
        cleanup_timeout: 3600
        # Loggers for the job service
        job_loggers:
          - "STD_OUTPUT"
          - "FILE"
          - "DB"
        # Logger sweeper duration (clean logs every 24 hours)
        logger_sweeper_duration: 24
        # Retry settings for failed jobs
        job_retry_count: 3
        job_retry_delay: 10
        # Enable job metrics collection
        enable_metrics: true
      
      notification:
        # Maximum retry count for webhook job
        webhook_job_max_retry: 10
        # HTTP client timeout for webhook jobs (in seconds)
        webhook_job_http_client_timeout: 10
      
      chart:
        # Change the value of absolute_url to enabled can enable absolute url in chart
        absolute_url: disabled
      
      # Log configurations
      log:
        # options are debug, info, warning, error, fatal
        level: info
        # configs for logs in local storage
        local:
          # Log files are rotated log_rotate_count times before being removed. If count is 0, old versions are removed rather than rotated.
          rotate_count: 50
          # Log files are rotated only if they grow bigger than log_rotate_size bytes. If size is followed by k, the size is assumed to be in kilobytes.
          # If the M is used, the size is in megabytes, and if G is used, the size is in gigabytes. So size 100, size 100k, size 100M and size 100G
          # are all valid.
          rotate_size: 200M
          # The directory on your host that store log
          location: /data/harbor/logs
      
      #This attribute is for migrator to detect the version of the .cfg file, DO NOT MODIFY!
      _version: 2.11.0
      
      # Uncomment external_database if using external database.
      # external_database:
      #   harbor:
      #     host: harbor_db_host
      #     port: harbor_db_port
      #     db_name: harbor_db_name
      #     username: harbor_db_username
      #     password: harbor_db_password
      #     ssl_mode: disable
      #     max_idle_conns: 2
      #     max_open_conns: 0
      #   notary_signer:
      #     host: notary_signer_db_host
      #     port: notary_signer_db_port
      #     db_name: notary_signer_db_name
      #     username: notary_signer_db_username
      #     password: notary_signer_db_password
      #     ssl_mode: disable
      #   notary_server:
      #     host: notary_server_db_host
      #     port: notary_server_db_port
      #     db_name: notary_server_db_name
      #     username: notary_server_db_username
      #     password: notary_server_db_password
      #     ssl_mode: disable
      
      # Uncomment external_redis if using external Redis server
      # external_redis:
      #   # support redis, redis+sentinel
      #   # host for redis: <host_redis>:<port_redis>
      #   # host for redis+sentinel:
      #   #  <host_sentinel1>:<port_sentinel1>,<host_sentinel2>:<port_sentinel2>,<host_sentinel3>:<port_sentinel3>
      #   host: redis:6379
      #   password:
      #   # sentinel_master_set must be set to support redis+sentinel
      #   #sentinel_master_set:
      #   # db_index 0 is for core, it's unchangeable
      #   # db_index 1 is for jobservice
      #   # db_index 2 is for trivy
      #   db_index: 0
      
      # Uncomment uaa for trusting the certificate of uaa instance that is hosted via self-signed cert.
      # uaa:
      #   ca_file: /path/to/ca
      
      # Global proxy
      # Config http proxy for components, e.g. http://my.proxy.com:3128
      # Components doesn't need to connect to each others via http proxy.
      # Remove component from `components` array if want disable proxy
      # for it. If you want use proxy for replication, MUST enable proxy
      # for core and jobservice, and set `http_proxy` and `https_proxy`.
      # Add domain to the `no_proxy` list when want disable proxy
      # for some special registry
      proxy:
        http_proxy: ""
        https_proxy: ""
        no_proxy: ""
        components:
          - core
          - jobservice
          - trivy
      
      # metric:
      #   enabled: false
      #   port: 9090
      #   path: /metrics
      
      # Trace related config
      # only can enable one trace at the same time.
      # trace:
      #   enabled: true
      #   # set sample_rate to 1 if you wanna sampling 100% of trace data; set 0.5 if you wanna sampling 50% of trace data, and so forth
      #   sample_rate: 1
      #   # # namespace used to differentiate different harbor services
      #   # namespace:
      #   # # attributes is a key value dict contains user defined attributes used to initialize trace provider
      #   # attributes:
      #   #   application: harbor
      #   # # jaeger should be 1.26 or newer.
      #   # jaeger:
      #   #   endpoint: http://hostname:14268/api/traces
      #   #   username:
      #   #   password:
      #   #   agent_host: hostname
      #   #   agent_port: 6832
      #   # otel:
      #   #   endpoint: hostname:4318
      #   #   url_path: /v1/traces
      #   #   compression: false
      #   #   insecure: true
      #   #   timeout: 10s
      
      # enable purge _upload directories
      upload_purging:
        enabled: true
        # remove files in _upload directories which exist for a period of time, default is one week.
        age: 168h
        # the interval of the purge operations
        interval: 24h
        dryrun: false
      
      # cache layer configurations
      # If this feature enabled, harbor will cache the resource
      # `project/project_metadata/repository/artifact/manifest` in the redis
      # which can especially help to improve the performance of high concurrent
      # manifest pulling.
      # NOTICE
      # If you are deploying Harbor in HA mode, make sure that all the harbor
      # instances have the same behaviour, all with caching enabled or disabled,
      # otherwise it can lead to potential data inconsistency.
      cache:
        # default is not enabled.
        enabled: false
        # default keep cache for one day.
        expire_hours: 24
      EOF
      
      # Create required directories for Harbor logging on dedicated disk
      mkdir -p /data/harbor/logs/jobs
      mkdir -p /data/harbor/logs
      
      # Install Harbor
      echo "üöÄ Installing Harbor..."
      /opt/harbor/install.sh --with-trivy
      
      # Wait for Harbor to start
      echo "‚è≥ Waiting for Harbor to start..."
      sleep 30
      
      # Check and fix jobservice if needed
      echo "üîß Checking Harbor jobservice health..."
      cd /opt/harbor
      
      # Wait for all Harbor services to be ready
      echo "‚è≥ Waiting for Harbor services to be ready..."
      while true; do
        if docker-compose ps | grep -q "Up" && docker-compose ps | grep -q "jobservice"; then
          echo "‚úÖ Harbor services are running"
          break
        fi
        echo "Waiting for Harbor services..."
        sleep 10
      done
      
      # Check jobservice specifically
      JOBSERVICE_STATUS=$(docker-compose ps harbor-jobservice | grep "Up" || echo "")
      if [ -z "$JOBSERVICE_STATUS" ]; then
        echo "‚ö†Ô∏è  Jobservice not running properly, restarting Harbor..."
        docker-compose down
        sleep 10
        docker-compose up -d
        sleep 30
        
        # Verify jobservice again
        RETRY_STATUS=$(docker-compose ps harbor-jobservice | grep "Up" || echo "")
        if [ -z "$RETRY_STATUS" ]; then
          echo "‚ùå Jobservice still failing, checking logs..."
          docker-compose logs harbor-jobservice
          echo "Attempting jobservice container restart..."
          docker-compose restart harbor-jobservice
          sleep 15
        else
          echo "‚úÖ Jobservice recovered after restart"
        fi
      else
        echo "‚úÖ Jobservice is running properly"
      fi
      
      # Configure Harbor as pull-through cache
      echo "üîß Configuring Harbor as pull-through cache..."
      
      # Create proxy cache projects for common registries
      HARBOR_HOST="utility-node.internal.lan"
      HARBOR_URL="http://$HARBOR_HOST"
      export HARBOR_USER=${HARBOR_USER}
      export HARBOR_PASS=${HARBOR_PASS}

      
      # Wait for Harbor API to be available with better timing
      echo "‚è≥ Waiting for Harbor API to be fully ready..."
      sleep 60  # Give Harbor more time to fully initialize
      
      until curl -f -s "$HARBOR_URL/api/v2.0/systeminfo" > /dev/null; do
        echo "Waiting for Harbor API to be available..."
        sleep 15
      done
      
      # Wait additional time for Harbor to be fully operational
      echo "‚úÖ Harbor API available, waiting for full initialization..."
      sleep 30
      
      # Function to create proxy cache project
      create_proxy_project() {
        local project_name=$1
        local registry_url=$2
        local registry_name=$3
        
        echo "Creating proxy cache project: $project_name"
        
        # Create registry endpoint first
        echo "Creating registry endpoint: $registry_name"
        REGISTRY_RESPONSE=$(curl -s -w "\n%%{http_code}" -X POST "$HARBOR_URL/api/v2.0/registries" \
          -H "Content-Type: application/json" \
          -u "${HARBOR_USER}:${HARBOR_PASS}" \
          -d "{
            \"name\": \"$registry_name\",
            \"type\": \"docker-registry\",
            \"url\": \"$registry_url\",
            \"insecure\": false,
            \"credential\": {
              \"type\": \"basic\",
              \"access_key\": \"\",
              \"access_secret\": \"\"
            }
          }")
        
        HTTP_CODE=$(echo "$REGISTRY_RESPONSE" | tail -n1)
        RESPONSE_BODY=$(echo "$REGISTRY_RESPONSE" | head -n -1)
        
        if [ "$HTTP_CODE" != "201" ] && [ "$HTTP_CODE" != "409" ]; then
          echo "‚ùå Failed to create registry $registry_name (HTTP $HTTP_CODE): $RESPONSE_BODY"
          return 1
        fi
        
        # Wait a moment for registry to be created
        sleep 5
        
        # Get registry ID
        echo "Getting registry ID for: $registry_name"
        REGISTRY_ID=""
        while true; do
          REGISTRY_ID=$(curl -s "$HARBOR_URL/api/v2.0/registries" \
            -u "${HARBOR_USER}:${HARBOR_PASS}" | \
            jq -r ".[] | select(.name==\"$registry_name\") | .id" 2>/dev/null)
          
          if [ ! -z "$REGISTRY_ID" ] && [ "$REGISTRY_ID" != "null" ]; then
            echo "‚úÖ Found registry ID: $REGISTRY_ID"
            break
          fi
          echo "Waiting for registry to be available..."
          sleep 3
        done
        
        if [ -z "$REGISTRY_ID" ] || [ "$REGISTRY_ID" == "null" ]; then
          echo "‚ùå Failed to get registry ID for $registry_name"
          return 1
        fi
        
        # Create proxy cache project
        echo "Creating proxy cache project: $project_name with registry ID: $REGISTRY_ID"
        PROJECT_RESPONSE=$(curl -s -w "\n%%{http_code}" -X POST "$HARBOR_URL/api/v2.0/projects" \
          -H "Content-Type: application/json" \
          -u "${HARBOR_USER}:${HARBOR_PASS}" \
          -d "{
            \"project_name\": \"$project_name\",
            \"registry_id\": $REGISTRY_ID,
            \"metadata\": {
              \"public\": \"true\"
            }
          }")
        
        PROJECT_HTTP_CODE=$(echo "$PROJECT_RESPONSE" | tail -n1)
        PROJECT_RESPONSE_BODY=$(echo "$PROJECT_RESPONSE" | head -n -1)
        
        if [ "$PROJECT_HTTP_CODE" != "201" ] && [ "$PROJECT_HTTP_CODE" != "409" ]; then
          echo "‚ùå Failed to create project $project_name (HTTP $PROJECT_HTTP_CODE): $PROJECT_RESPONSE_BODY"
          return 1
        fi
        
        echo "‚úÖ Successfully created proxy cache project: $project_name"
        sleep 2  # Brief pause between projects
      }
      
      # Create proxy cache projects for popular registries
      create_proxy_project "dockerhub" "https://registry-1.docker.io" "docker-hub"
      create_proxy_project "gcr" "https://gcr.io" "google-gcr"
      create_proxy_project "quay" "https://quay.io" "quay-io"
      create_proxy_project "k8s-gcr" "https://k8s.gcr.io" "k8s-gcr-io"
      create_proxy_project "registry-k8s" "https://registry.k8s.io" "registry-k8s-io"
      
      # Configure GitHub OAuth authentication automatically
      echo "üîß Configuring GitHub OAuth authentication..."
      
      # Verify environment variables are set
      if [ -z "${HARBOR_OIDC_CLIENT}" ] || [ -z "${HARBOR_OIDC_SECRET}" ]; then
        echo "‚ùå GitHub OAuth credentials not provided (HARBOR_OIDC_CLIENT or HARBOR_OIDC_SECRET missing)"
        echo "‚ö†Ô∏è  Keeping database authentication mode"
      else
        echo "‚úÖ GitHub OAuth credentials found"
        echo "üîß Client ID: ${HARBOR_OIDC_CLIENT}"
        echo "üîß Using endpoint: https://github.com"
        
        # Wait for Harbor to be fully ready before configuring OAuth
        sleep 15
        
        # GitHub OAuth configuration via Harbor API
        echo "Setting up GitHub OAuth with Harbor API..."
      
      # Apply GitHub OAuth configuration to Harbor
      OAUTH_RESPONSE=$(curl -s -w "\\n%%{http_code}" -X PUT "$HARBOR_URL/api/v2.0/configurations" -H "Content-Type: application/json" -u "${HARBOR_USER}:${HARBOR_PASS}" -d '{
          "auth_mode": "oidc_auth",
          "oidc_name": "GitHub",
          "oidc_endpoint": "https://github.com",
          "oidc_client_id": "'${HARBOR_OIDC_CLIENT}'",
          "oidc_client_secret": "'${HARBOR_OIDC_SECRET}'",
          "oidc_scope": "read:user,user:email",
          "oidc_verify_cert": true,
          "oidc_auto_onboard": true,
          "oidc_user_claim": "login",
          "oidc_groups_claim": "",
          "oidc_admin_group": "",
          "oidc_group_filter": ""
        }')
      
      OAUTH_HTTP_CODE=$(echo "$OAUTH_RESPONSE" | tail -n1)
      OAUTH_RESPONSE_BODY=$(echo "$OAUTH_RESPONSE" | head -n -1)
      
      if [ "$OAUTH_HTTP_CODE" = "200" ]; then
        echo "‚úÖ GitHub OAuth configuration applied successfully"
        
        # Wait for configuration to take effect
        sleep 10
        
        # Verify OAuth configuration
        echo "Verifying GitHub OAuth configuration..."
        AUTH_MODE=$(curl -s "$HARBOR_URL/api/v2.0/configurations" -u "${HARBOR_USER}:${HARBOR_PASS}" | jq -r '.auth_mode.value // .auth_mode' 2>/dev/null)
        
        if [ "$AUTH_MODE" = "oidc_auth" ]; then
          echo "‚úÖ GitHub OAuth authentication is now active"
        else
          echo "‚ö†Ô∏è  Auth mode is $AUTH_MODE - GitHub OAuth may need verification"
        fi
      else
        echo "‚ùå Failed to configure GitHub OAuth (HTTP $OAUTH_HTTP_CODE)"
        echo "‚ùå Error response: $OAUTH_RESPONSE_BODY"
        echo "üîß Attempting alternative configuration with simplified settings..."
        
        # Alternative: Configure with minimal OIDC settings
        RETRY_RESPONSE=$(curl -s -w "\\n%%{http_code}" -X PUT "$HARBOR_URL/api/v2.0/configurations" -H "Content-Type: application/json" -u "${HARBOR_USER}:${HARBOR_PASS}" -d '{
            "auth_mode": "oidc_auth",
            "oidc_name": "GitHub",
            "oidc_endpoint": "https://github.com",
            "oidc_client_id": "'${HARBOR_OIDC_CLIENT}'",
            "oidc_client_secret": "'${HARBOR_OIDC_SECRET}'",
            "oidc_scope": "read:user",
            "oidc_verify_cert": false,
            "oidc_auto_onboard": true,
            "oidc_user_claim": "login"
          }')
        
        RETRY_HTTP_CODE=$(echo "$RETRY_RESPONSE" | tail -n1)
        if [ "$RETRY_HTTP_CODE" = "200" ]; then
          echo "‚úÖ GitHub OAuth configured with simplified settings"
        else
          echo "‚ùå GitHub OAuth configuration failed - keeping database auth"
          # Reset to database auth if OAuth fails
          curl -s -X PUT "$HARBOR_URL/api/v2.0/configurations" -H "Content-Type: application/json" -u "${HARBOR_USER}:${HARBOR_PASS}" -d '{"auth_mode": "db_auth"}' || true
        fi
      fi
      fi
      
      # Create user groups for GitHub teams
      echo "üë• Creating user groups for GitHub teams..."
      
      # Create harbor_admins group
      echo "Creating harbor_admins group..."
      ADMIN_GROUP_RESPONSE=$(curl -s -w "\n%%{http_code}" -X POST "$HARBOR_URL/api/v2.0/usergroups" \
        -H "Content-Type: application/json" \
        -u "${HARBOR_USER}:${HARBOR_PASS}" \
        -d "{
          \"group_name\": \"harbor_admins\",
          \"group_type\": 2,
          \"ldap_group_dn\": \"harbor_admins\"
        }")
      
      ADMIN_HTTP_CODE=$(echo "$ADMIN_GROUP_RESPONSE" | tail -n1)
      if [ "$ADMIN_HTTP_CODE" != "201" ] && [ "$ADMIN_HTTP_CODE" != "409" ]; then
        echo "‚ö†Ô∏è  Failed to create harbor_admins group (HTTP $ADMIN_HTTP_CODE)"
      else
        echo "‚úÖ harbor_admins group created"
      fi
      
      # Create harbor_users group  
      echo "Creating harbor_users group..."
      USER_GROUP_RESPONSE=$(curl -s -w "\n%%{http_code}" -X POST "$HARBOR_URL/api/v2.0/usergroups" \
        -H "Content-Type: application/json" \
        -u "${HARBOR_USER}:${HARBOR_PASS}" \
        -d "{
          \"group_name\": \"harbor_users\",
          \"group_type\": 2,
          \"ldap_group_dn\": \"harbor_users\"
        }")
      
      USER_HTTP_CODE=$(echo "$USER_GROUP_RESPONSE" | tail -n1)
      if [ "$USER_HTTP_CODE" != "201" ] && [ "$USER_HTTP_CODE" != "409" ]; then
        echo "‚ö†Ô∏è  Failed to create harbor_users group (HTTP $USER_HTTP_CODE)"
      else
        echo "‚úÖ harbor_users group created"
      fi
      
      # Get group IDs for role assignment
      echo "Getting group IDs for role assignment..."
      sleep 5  # Allow groups to be fully created
      
      ADMIN_GROUP_ID=$(curl -s "$HARBOR_URL/api/v2.0/usergroups" \
        -u "${HARBOR_USER}:${HARBOR_PASS}" | \
        jq -r '.[] | select(.group_name=="harbor_admins") | .id' 2>/dev/null)
      
      USER_GROUP_ID=$(curl -s "$HARBOR_URL/api/v2.0/usergroups" \
        -u "${HARBOR_USER}:${HARBOR_PASS}" | \
        jq -r '.[] | select(.group_name=="harbor_users") | .id' 2>/dev/null)
      
      echo "Admin Group ID: $ADMIN_GROUP_ID"
      echo "User Group ID: $USER_GROUP_ID"
      
      # Add harbor_admins group as system admin (to library project - project ID 1)
      if [ ! -z "$ADMIN_GROUP_ID" ] && [ "$ADMIN_GROUP_ID" != "null" ]; then
        echo "Adding harbor_admins group to library project with admin role..."
        curl -s -w "\n%%{http_code}" -X POST "$HARBOR_URL/api/v2.0/projects/1/members" \
          -H "Content-Type: application/json" \
          -u "${HARBOR_USER}:${HARBOR_PASS}" \
          -d "{
            \"member_group\": {
              \"id\": $ADMIN_GROUP_ID,
              \"group_name\": \"harbor_admins\"
            },
            \"role_id\": 1
          }" | tail -n1 | {
            read http_code
            if [ "$http_code" = "201" ] || [ "$http_code" = "409" ]; then
              echo "‚úÖ harbor_admins group added to library project"
            else
              echo "‚ö†Ô∏è  Failed to add harbor_admins to library project (HTTP $http_code)"
            fi
          }
      else
        echo "‚ö†Ô∏è  Could not find harbor_admins group ID"
      fi
      
      # Add harbor_users group with limited read access to proxy cache projects
      if [ ! -z "$USER_GROUP_ID" ] && [ "$USER_GROUP_ID" != "null" ]; then
        echo "Adding harbor_users group to proxy cache projects..."
        for project in dockerhub gcr quay k8s-gcr registry-k8s; do
          echo "Adding harbor_users to project: $project"
          PROJECT_ID=$(curl -s "$HARBOR_URL/api/v2.0/projects?name=$project" \
            -u "${HARBOR_USER}:${HARBOR_PASS}" | \
            jq -r '.[0].project_id // empty' 2>/dev/null)
          
          if [ ! -z "$PROJECT_ID" ] && [ "$PROJECT_ID" != "null" ]; then
            curl -s -w "\n%%{http_code}" -X POST "$HARBOR_URL/api/v2.0/projects/$PROJECT_ID/members" \
              -H "Content-Type: application/json" \
              -u "${HARBOR_USER}:${HARBOR_PASS}" \
              -d "{
                \"member_group\": {
                  \"id\": $USER_GROUP_ID,
                  \"group_name\": \"harbor_users\"
                },
                \"role_id\": 3
              }" | tail -n1 | {
                read http_code
                if [ "$http_code" = "201" ] || [ "$http_code" = "409" ]; then
                  echo "‚úÖ harbor_users group added to $project project"
                else
                  echo "‚ö†Ô∏è  Failed to add harbor_users to $project project (HTTP $http_code)"
                fi
              }
          else
            echo "‚ö†Ô∏è  Could not find project ID for $project"
          fi
        done
      else
        echo "‚ö†Ô∏è  Could not find harbor_users group ID"
      fi
      
      echo "‚úÖ GitHub OAuth authentication configured"
      echo "üë• User groups configured:"
      echo "  - harbor_admins: System administrators"
      echo "  - harbor_users: Read-only access to proxy cache projects"
      
      # Create systemd service for Harbor management
      cat <<EOF > /etc/systemd/system/harbor.service
      [Unit]
      Description=Harbor
      After=docker.service systemd-networkd.service systemd-resolved.service
      Requires=docker.service
      Documentation=http://github.com/goharbor/harbor
      
      [Service]
      Type=oneshot
      ExecStart=/opt/harbor/prepare && /usr/bin/docker-compose -f /opt/harbor/docker-compose.yml up -d
      ExecStop=/usr/bin/docker-compose -f /opt/harbor/docker-compose.yml down
      WorkingDirectory=/opt/harbor
      RemainAfterExit=yes
      
      [Install]
      WantedBy=multi-user.target
      EOF
      
      systemctl daemon-reload
      systemctl enable harbor
      
      # Create helper script for Harbor management
      cat <<EOF > /usr/local/bin/harbor-manage
      #!/bin/bash
      cd /opt/harbor
      case "\$1" in
        start)
          docker-compose up -d
          ;;
        stop)
          docker-compose down
          ;;
        restart)
          docker-compose down
          docker-compose up -d
          ;;
        status)
          docker-compose ps
          ;;
        logs)
          docker-compose logs -f
          ;;
        jobservice-restart)
          echo "Restarting Harbor jobservice..."
          docker-compose restart harbor-jobservice
          sleep 10
          docker-compose ps harbor-jobservice
          ;;
        jobservice-logs)
          echo "Harbor jobservice logs:"
          docker-compose logs -f harbor-jobservice
          ;;
        jobservice-status)
          echo "Harbor jobservice status:"
          docker-compose ps harbor-jobservice
          docker-compose exec harbor-jobservice ps aux | grep -E "(job|worker)" || echo "No jobservice processes found"
          ;;
        *)
          echo "Usage: \$0 {start|stop|restart|status|logs|jobservice-restart|jobservice-logs|jobservice-status}"
          exit 1
          ;;
      esac
      EOF
      
      chmod +x /usr/local/bin/harbor-manage
      
      # Create jobservice monitoring script
      cat <<EOF > /usr/local/bin/harbor-jobservice-monitor
      #!/bin/bash
      # Harbor jobservice health monitoring script
      
      HARBOR_DIR="/opt/harbor"
      LOG_FILE="/var/log/harbor-jobservice-monitor.log"
      
      cd "\$HARBOR_DIR"
      
      # Function to log with timestamp
      log_message() {
        echo "\$(date): \$1" | tee -a "\$LOG_FILE"
      }
      
      # Check if jobservice container is running
      JOBSERVICE_RUNNING=\$(docker-compose ps harbor-jobservice | grep -c "Up")
      
      if [ "\$JOBSERVICE_RUNNING" -eq 0 ]; then
        log_message "ALERT: Harbor jobservice is not running"
        
        # Try to restart jobservice
        log_message "Attempting to restart harbor-jobservice..."
        docker-compose restart harbor-jobservice
        sleep 15
        
        # Check again
        JOBSERVICE_RUNNING_RETRY=\$(docker-compose ps harbor-jobservice | grep -c "Up")
        if [ "\$JOBSERVICE_RUNNING_RETRY" -eq 0 ]; then
          log_message "CRITICAL: Harbor jobservice failed to restart"
          # Log the container status and logs
          docker-compose ps harbor-jobservice >> "\$LOG_FILE"
          docker-compose logs --tail=50 harbor-jobservice >> "\$LOG_FILE"
        else
          log_message "SUCCESS: Harbor jobservice restarted successfully"
        fi
      else
        log_message "OK: Harbor jobservice is running normally"
      fi
      
      # Check jobservice queue depth (if accessible)
      QUEUE_DEPTH=\$(docker-compose exec -T harbor-core redis-cli -h redis llen "jobservice_namespace:job_queue" 2>/dev/null || echo "N/A")
      log_message "Jobservice queue depth: \$QUEUE_DEPTH"
      
      # Memory usage check
      JOBSERVICE_MEMORY=\$(docker stats --no-stream --format "table {{.Container}}\t{{.MemUsage}}" | grep jobservice || echo "N/A")
      log_message "Jobservice memory usage: \$JOBSERVICE_MEMORY"
      EOF
      
      chmod +x /usr/local/bin/harbor-jobservice-monitor
      
      # Create systemd timer for jobservice monitoring
      cat <<EOF > /etc/systemd/system/harbor-jobservice-monitor.service
      [Unit]
      Description=Harbor JobService Health Monitor
      After=harbor.service
      
      [Service]
      Type=oneshot
      ExecStart=/usr/local/bin/harbor-jobservice-monitor
      User=root
      EOF
      
      cat <<EOF > /etc/systemd/system/harbor-jobservice-monitor.timer
      [Unit]
      Description=Run Harbor JobService Health Monitor every 5 minutes
      Requires=harbor-jobservice-monitor.service
      
      [Timer]
      OnCalendar=*:0/5
      RandomizedDelaySec=30
      
      [Install]
      WantedBy=timers.target
      EOF
      
      systemctl daemon-reload
      systemctl enable harbor-jobservice-monitor.timer
      systemctl start harbor-jobservice-monitor.timer
      
      echo "‚úÖ Harbor installation completed!"
      echo "üåê Harbor is available at: http://$HARBOR_HOST"
      echo "üåê External Harbor URL: https://harbor.${RANCHER_DOMAIN}"
      echo "üîê GitHub OAuth authentication configured automatically"
      echo "üë§ Default admin credentials (if OAuth fails): admin / Harbor12345"
      echo "ÔøΩ GitHub OIDC Authentication configured"
      echo "üë§ GitHub Teams configured:"
      echo "  - harbor_admins: Full system administrative access"
      echo "  - harbor_users: Read-only access to proxy cache projects"
      echo ""
      echo "‚ö†Ô∏è  IMPORTANT: After first GitHub OIDC login, disable the built-in admin account:"
      echo "   1. Login via GitHub OIDC as a member of harbor_admins team"
      echo "   2. Go to Administration > Users"
      echo "   3. Disable the 'admin' user account"
      echo ""
      echo "üîß Use 'harbor-manage' command to manage Harbor service"
      echo ""
      echo "üìã Proxy cache projects created:"
      echo "  - dockerhub (Docker Hub): $HARBOR_HOST/dockerhub/"
      echo "  - gcr (Google Container Registry): $HARBOR_HOST/gcr/"
      echo "  - quay (Quay.io): $HARBOR_HOST/quay/"
      echo "  - k8s-gcr (Kubernetes GCR): $HARBOR_HOST/k8s-gcr/"
      echo "  - registry-k8s (Kubernetes Registry): $HARBOR_HOST/registry-k8s/"
      echo ""
      echo "üöÄ Configure your Kubernetes cluster to use Harbor as pull-through cache:"
      echo "  Example: docker pull $HARBOR_HOST/dockerhub/nginx:latest"

  - path: /usr/local/bin/performance-tuning.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      echo "üîß Applying performance tuning..."
      
      # CPU Governor
      GOVERNOR_PATH="/sys/devices/system/cpu/cpu0/cpufreq/scaling_governor"
      if [ -f "$GOVERNOR_PATH" ]; then
        for CPU in /sys/devices/system/cpu/cpu[0-9]*; do
          echo performance > "$CPU/cpufreq/scaling_governor" 2>/dev/null || true
        done
        echo "‚úÖ CPU governors set to performance"
      else
        echo "‚ö†Ô∏è CPU frequency scaling not available ‚Äî skipping governor tuning"
      fi
      
      # Scheduler tuning for SSDs
      for dev in /sys/block/sd*/queue/scheduler; do
        echo mq-deadline > "$dev"
      done

      systemctl disable apt-daily.timer
      systemctl disable snapd
      systemctl disable motd-news.timer
      
      # Sysctl parameters
      cat <<EOF > /etc/sysctl.d/99-performance-tuning.conf
      fs.inotify.max_user_watches=524288
      vm.max_map_count=262144
      net.core.rmem_max=134217728
      net.core.wmem_max=134217728
      net.ipv4.tcp_rmem=4096 87380 134217728
      net.ipv4.tcp_wmem=4096 65536 134217728
      net.ipv4.tcp_congestion_control=bbr
      vm.swappiness=10
      vm.dirty_ratio=15
      vm.dirty_background_ratio=5
      EOF
      
      sysctl --system

      # 2) Pick all ‚Äúlarge-MTU‚Äù interfaces automatically
      #    (adjust the grep if your jumbo-MTU devices have a different naming scheme)
      for IF in $(ip -o link show | awk -F': ' '{print $2}' | grep -E '^(eth|vtnet)'); do
        # bring MTU up before offloads
        ip link set dev $IF mtu 9000

        # enable big-packet offloads
        ethtool -K $IF gro on gso on tso on rx on tx on

        # increase the kernel TX queue length
        ip link set dev $IF txqueuelen 10000
      done

      # 3) Sysctls ‚Äî bump backlog and buffers for all interfaces
      #    and enable TCP MTU probing (helps path-MTU discovery)
      sysctl -w net.core.netdev_max_backlog=250000
      sysctl -w net.core.rmem_max=134217728
      sysctl -w net.core.wmem_max=134217728
      sysctl -w net.ipv4.tcp_rmem="4096 87380 134217728"
      sysctl -w net.ipv4.tcp_wmem="4096 87380 134217728"
      sysctl -w net.ipv4.tcp_mtu_probing=1

      # 4) Persist sysctls
      cat <<EOF >/etc/sysctl.d/99-network-opt.conf
      net.core.netdev_max_backlog=250000
      net.core.rmem_max=134217728
      net.core.wmem_max=134217728
      net.ipv4.tcp_rmem=4096 87380 134217728
      net.ipv4.tcp_wmem=4096 87380 134217728
      net.ipv4.tcp_mtu_probing=1
      EOF

      # 5) Persist ethtool + txqueuelen via a systemd service
      cat <<'EOF' >/etc/systemd/system/network-tweaks.service
      [Unit]
      Description=Apply ethtool offloads + txqueuelen
      After=network-online.target
      Wants=network-online.target

      [Service]
      Type=oneshot
      ExecStart=/usr/local/bin/network-tweaks.sh

      [Install]
      WantedBy=multi-user.target
      EOF

      # Install the same script to /usr/local/bin and make it executable
            cat <<'EOF' >/usr/local/bin/network-tweaks.sh
      #!/bin/bash
      set -e
      for IF in $(ip -o link show | awk -F': ' '{print $2}' | grep -E '^(eth|vtnet)'); do
        CURRENT_MTU=$(ip -o link show "$IF" | awk '{print $4}')
        if [ "$CURRENT_MTU" != "9000" ]; then
          ip link set dev "$IF" mtu 9000
        fi
        ethtool -K $IF gro on gso on tso on rx on tx on
        ip link set dev $IF txqueuelen 10000
      done
      EOF

      chmod +x /usr/local/bin/network-tweaks.sh

      systemctl daemon-reload
      systemctl enable network-tweaks

      echo "‚úÖ Network tweaks applied and will persist across reboots."
      
      # Limit nofile
      cat <<EOF > /etc/security/limits.d/99-nofile.conf
      * soft nofile 1048576
      * hard nofile 1048576
      EOF
      
      # Systemd service overrides
      mkdir -p /etc/systemd/system/containerd.service.d
      mkdir -p /etc/systemd/system/rke2-server.service.d
      mkdir -p /etc/systemd/system/rke2-agent.service.d
      
      cat <<EOF > /etc/systemd/system/containerd.service.d/override.conf
      [Service]
      LimitNOFILE=1048576
      LimitNPROC=65536
      TasksMax=infinity
      EOF
      
      cat <<EOF > /etc/systemd/system/rke2-server.service.d/override.conf
      [Service]
      LimitNOFILE=1048576
      LimitNPROC=65536
      TasksMax=infinity
      EOF
      
      cat <<EOF > /etc/systemd/system/rke2-agent.service.d/override.conf
      [Service]
      LimitNOFILE=1048576
      LimitNPROC=65536
      TasksMax=infinity
      EOF

      # cat <<EOF | sudo tee /etc/profile.d/http_proxy.sh
      # export HTTP_PROXY="http://10.0.0.10:3128"
      # export HTTPS_PROXY="http://10.0.0.10:3128"
      # export NO_PROXY="127.0.0.1,localhost,10.0.0.0/24,10.96.0.0/12,192.168.0.0/16"
      # EOF

      # sudo chmod +x /etc/profile.d/http_proxy.sh

      # Unbind kernel workqueues from specific CPUs (recommended on NUMA/multicore systems)
      if [ -e /sys/module/workqueue/parameters/disable_bound ]; then
        echo y > /sys/module/workqueue/parameters/disable_bound
        echo "‚úÖ Enabled unbound kernel workqueues for better scalability"
      fi

      # 6) Add GRUB performance kernel parameters
      echo "‚úÖ Adding performance kernel parameters to GRUB..."
      GRUB_CONFIG_FILE="/etc/default/grub"

      # Append only if not already present
      if ! grep -q "workqueue.disable_bound=1" "$GRUB_CONFIG_FILE"; then
        sed -i 's/GRUB_CMDLINE_LINUX_DEFAULT="/GRUB_CMDLINE_LINUX_DEFAULT="workqueue.disable_bound=1 intel_pstate=disable processor.max_cstate=1 idle=poll transparent_hugepage=never mitigations=off /' "$GRUB_CONFIG_FILE"
        update-grub
        echo "‚úÖ GRUB updated. Kernel parameters will apply on next boot."
      else
        echo "‚ö†Ô∏è GRUB already configured. Skipping update."
      fi
      
      systemctl daemon-reload

  - path: /usr/local/bin/setup-hosts-dns.sh
    permissions: '0755'
    content: |
      #!/bin/bash

      # Add hostname to /etc/hosts without modifying network config
      HOSTNAME=$(hostname)
      grep -q "127.0.1.1 $HOSTNAME" /etc/hosts || echo "127.0.1.1 $HOSTNAME" >> /etc/hosts

      # ---------------------------------------------
      # üßπ Disable systemd-resolved to free port 53
      # ---------------------------------------------
      echo "üßπ Disabling systemd-resolved..."
      sudo systemctl stop systemd-resolved || true
      sudo systemctl disable systemd-resolved || true
      sudo rm -f /etc/resolv.conf
      echo "nameserver 127.0.0.1" | sudo tee /etc/resolv.conf > /dev/null

      # ---------------------------------------------
      # ‚öôÔ∏è Configure dnsmasq split-DNS
      # ---------------------------------------------
      sudo tee /etc/dnsmasq.d/k8s-split-dns.conf > /dev/null <<EOF
      # Forward Kubernetes service lookups to CoreDNS
      server=/svc.cluster.local/10.43.0.10

      # Forward internal LAN lookups to LAN DNS
      server=/internal.lan/10.0.0.10

      # Forward everything else to LAN DNS
      server=10.0.0.10

      # Listen locally
      listen-address=127.0.0.1

      # Don't use /etc/resolv.conf
      no-resolv
      EOF

      # Ensure dnsmasq global config allows localhost
      sudo sed -i 's/^#\?listen-address=.*/listen-address=127.0.0.1/' /etc/dnsmasq.conf || echo 'listen-address=127.0.0.1' | sudo tee -a /etc/dnsmasq.conf

      sudo systemctl enable dnsmasq
      sudo systemctl restart dnsmasq

      # Confirm DNS server
      echo "‚úÖ dnsmasq is resolving:"
      echo "  - *.svc.cluster.local ‚Üí 10.43.0.10"
      echo "  - *.internal.lan      ‚Üí 10.0.0.10"
      echo "  - all others          ‚Üí 10.0.0.10"

      # ---------------------------------------------
      # üõ° IPv6 Hard Disable (Optional)
      # ---------------------------------------------
      sudo modprobe -r ip6_tables || true
      sudo modprobe -r nf_defrag_ipv6 || true
      sudo modprobe -r nf_conntrack_ipv6 || true
      sudo modprobe -r ipv6 || true

      echo "install ipv6 /bin/true" | sudo tee /etc/modprobe.d/force-disable-ipv6.conf
      echo "install ip6_tables /bin/true" | sudo tee -a /etc/modprobe.d/force-disable-ipv6.conf

      if [ -e /usr/sbin/ip6tables ]; then
        sudo mv /usr/sbin/ip6tables /usr/sbin/ip6tables.disabled
      fi

      sudo tee /etc/modprobe.d/disable-ipv6.conf > /dev/null <<EOF
      blacklist ipv6
      blacklist nf_conntrack_ipv6
      blacklist nf_defrag_ipv6
      blacklist ip6_tables
      EOF

      sudo update-initramfs -u

      # ---------------------------------------------
      # üîÅ Force sysctl re-apply on boot
      # ---------------------------------------------
      sudo tee /etc/systemd/system/sysctl-ensure.service > /dev/null <<EOF
      [Unit]
      Description=Force re-apply sysctl settings
      After=network.target
      Wants=network.target

      [Service]
      Type=oneshot
      ExecStart=/sbin/sysctl --system

      [Install]
      WantedBy=multi-user.target
      EOF

      sudo systemctl daemon-reexec
      sudo systemctl enable sysctl-ensure.service

  - path: /etc/systemd/system/setup-hosts-dns.service
    permissions: '0644'
    content: |
      [Unit]
      Description=Setup hosts and DNS configuration
      After=network-online.target
      Wants=network-online.target

      [Service]
      Type=oneshot
      ExecStart=/usr/local/bin/setup-hosts-dns.sh
      RemainAfterExit=true

      [Install]
      WantedBy=multi-user.target

  - path: /etc/sysctl.d/99-disable-ipv6.conf
    permissions: '0644'
    content: |
      net.ipv6.conf.all.disable_ipv6 = 1
      net.ipv6.conf.default.disable_ipv6 = 1
      net.ipv6.conf.lo.disable_ipv6 = 1

  - path: /etc/default/grub.d/99-disable-ipv6.cfg
    permissions: '0644'
    content: |
      GRUB_CMDLINE_LINUX="$GRUB_CMDLINE_LINUX ipv6.disable=1"

  - path: /usr/local/bin/disable-swap.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      swapoff -a
      cp /etc/fstab /etc/fstab.bak
      sed -i '/^[^#].*\bswap\b/s/^/#/' /etc/fstab
      echo fs.inotify.max_user_watches=524288 | sudo tee /etc/sysctl.d/99-inotify.conf

  - path: /usr/local/bin/setup-docker.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      until curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -; do sleep 5; done
      sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
      sudo apt-get update -y
      DEBIAN_FRONTEND=noninteractive sudo apt-get install -y docker-ce docker-ce-cli containerd.io

      # Install docker-compose immediately after Docker
      echo "üì¶ Installing docker-compose..."
      curl -L "https://github.com/docker/compose/releases/download/v2.24.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
      chmod +x /usr/local/bin/docker-compose
      
      # Verify docker-compose installation
      docker-compose --version
      echo "‚úÖ Docker and docker-compose installed successfully"

      # sudo mkdir -p /etc/systemd/system/docker.service.d
      # cat <<EOF | sudo tee /etc/systemd/system/docker.service.d/10-http-proxy.conf
      # [Service]
      # Environment="HTTP_PROXY=http://10.0.0.10:3128" "HTTPS_PROXY=http://10.0.0.10:3128" "NO_PROXY=127.0.0.1,localhost,10.0.0.0/24,10.96.0.0/12"
      # EOF

      # sudo mkdir -p /etc/systemd/system/containerd.service.d
      # cat <<EOF | sudo tee /etc/systemd/system/containerd.service.d/10-http-proxy.conf
      # [Service]
      # Environment="HTTP_PROXY=http://10.0.0.10:3128" "HTTPS_PROXY=http://10.0.0.10:3128" "NO_PROXY=127.0.0.1,localhost,10.0.0.0/24,10.96.0.0/12"
      # EOF

  - path: /etc/security/limits.d/99-nofile.conf
    permissions: '0644'
    content: |
      * soft nofile 1048576
      * hard nofile 1048576

  - path: /etc/systemd/system/containerd.service.d/override.conf
    permissions: '0644'
    content: |
      [Service]
      LimitNOFILE=1048576
      LimitNPROC=65536
      TasksMax=infinity

  - path: /etc/systemd/system/rke2-server.service.d/override.conf
    permissions: '0644'
    content: |
      [Service]
      LimitNOFILE=1048576
      LimitNPROC=65536
      TasksMax=infinity

  - path: /etc/systemd/system/rke2-agent.service.d/override.conf
    permissions: '0644'
    content: |
      [Service]
      LimitNOFILE=1048576
      LimitNPROC=65536
      TasksMax=infinity

  - path: /usr/local/bin/setup-fluentbit.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      set -e
      echo "üì¶ Configuring Fluent Bit for GCP logging..."

      mkdir -p /etc/google-cloud-ops-agent
      echo "${GCP_LOGGING_KEY}" | base64 -d > /etc/google-cloud-ops-agent/logging-key.json

      mkdir -p /usr/share/keyrings
      KEYRING_PATH="/usr/share/keyrings/fluent-bit-archive-keyring.gpg"

      # Clean up any broken or partial key
      rm -f "$KEYRING_PATH"

      until curl -fsSL https://packages.fluentbit.io/fluentbit.key | gpg --dearmor --no-tty --batch -o "$KEYRING_PATH"; do
        echo "Waiting for DNS resolution or valid key for fluentbit.key..."
        sleep 5
        rm -f "$KEYRING_PATH"  # ensure no corrupted key is left behind
      done

      DISTRO=$(lsb_release -cs)
      if [ "$DISTRO" = "${UBUNTU_RELEASE_CODE_NAME}" ]; then
        echo "Detected codename $DISTRO unsupported, falling back to jammy"
        DISTRO="jammy"
      fi

      echo "deb [signed-by=/usr/share/keyrings/fluent-bit-archive-keyring.gpg] https://packages.fluentbit.io/ubuntu/$DISTRO $DISTRO main" > /etc/apt/sources.list.d/fluent-bit.list
      sudo apt-get update
      sudo apt-get install -y td-agent-bit

      mkdir -p /etc/td-agent-bit/conf.d

      cat <<EOF > /etc/td-agent-bit/td-agent-bit.conf
      [SERVICE]
          flush         1
          daemon        Off
          log_level     info
          plugins_file  plugins.conf
          parsers_file  parsers.conf
          http_server   Off
          http_listen   0.0.0.0
          http_port     2020
          storage.metrics on

      @INCLUDE /etc/td-agent-bit/conf.d/*.conf
      EOF

      # cloud-init logs
      cat <<EOF > /etc/td-agent-bit/conf.d/cloudinit.conf
      [INPUT]
          Name               tail
          Mem_Buf_Limit     10MB
          Path               /var/log/cloud-init*.log
          Tag                cloudinit
          Read_from_Head     Off
          DB                 /var/log/flb_cloudinit.db
          DB.Sync            normal
          Skip_Long_Lines    On

      [FILTER]
          Name      parser
          Match     cloudinit
          Key_Name  log
          Parser    cloudinit_parser
          Reserve_Data true
          Preserve_Key true

      [FILTER]
          Name modify
          Match cloudinit
          Add logName cloudinit

      [FILTER]
          Name    nest
          Match   cloudinit
          Operation nest
          Wildcard process
          Wildcard pid
          Wildcard timestamp
          Nested_under logging.googleapis.com/labels

      [OUTPUT]
          Name  stackdriver
          Match cloudinit
          google_service_credentials /etc/google-cloud-ops-agent/logging-key.json
          resource ${MONITORED_RESOURCE_TYPE}
          location ${MONITORED_RESOURCE_LOCATION}
          namespace ${MONITORED_RESOURCE_NAMESPACE}
          node_id ${MONITORED_RESOURCE_NODE_ID}
          log_name_key cloudinit
      EOF

      # journal logs (systemd, rke2, etc.)
      cat <<EOF > /etc/td-agent-bit/conf.d/journald.conf
      [INPUT]
          Name    systemd
          Tag     journal.*
          Mem_Buf_Limit     10MB
          DB      /var/log/flb_journal.db
          Systemd_Filter  _SYSTEMD_UNIT=bootstrap-rke2.service
          Systemd_Filter  _SYSTEMD_UNIT=rke2-server.service
          Systemd_Filter  _SYSTEMD_UNIT=cloud-final.service
          Read_From_Tail  On

      [FILTER]
          Name      parser
          Match     journal.*
          Key_Name  MESSAGE
          Parser    journald_parser
          Reserve_Data true
          Preserve_Key true

      [FILTER]
          Name modify
          Match journal.*
          Add logName systemd-journal

      [FILTER]
          Name   modify
          Match  journal.*
          Rename message textPayload

      [FILTER]
          Name    nest
          Match   journal.*
          Operation nest
          Wildcard identifier
          Wildcard pid
          Wildcard hostname
          Nested_under logging.googleapis.com/labels

      [OUTPUT]
          Name  stackdriver
          Match journal.*
          google_service_credentials /etc/google-cloud-ops-agent/logging-key.json
          resource ${MONITORED_RESOURCE_TYPE}
          location ${MONITORED_RESOURCE_LOCATION}
          namespace ${MONITORED_RESOURCE_NAMESPACE}
          node_id ${MONITORED_RESOURCE_NODE_ID}
          log_name_key systemd-journal
      EOF

      # Optional: catch all syslog (if rsyslog is enabled)
      cat <<EOF > /etc/td-agent-bit/conf.d/syslog.conf
      [INPUT]
          Name              tail
          Mem_Buf_Limit     10MB
          Path              /var/log/syslog
          Tag               syslog
          Read_from_Head    Off
          DB                /var/log/flb_syslog.db
          Skip_Long_Lines   On

      [FILTER]
          Name   parser
          Match  syslog
          Key_Name log
          Parser syslog_parser
          Reserve_Data true
          Preserve_Key true

      [FILTER]
          Name   modify
          Match  syslog
          Rename message textPayload

      [FILTER]
          Name modify
          Match syslog
          Add logName syslog

      [FILTER]
          Name    nest
          Match   syslog
          Operation nest
          Wildcard hostname
          Wildcard identifier
          Wildcard pid
          Nested_under logging.googleapis.com/labels

      [OUTPUT]
          Name  stackdriver
          Match syslog
          google_service_credentials /etc/google-cloud-ops-agent/logging-key.json
          resource ${MONITORED_RESOURCE_TYPE}
          location ${MONITORED_RESOURCE_LOCATION}
          namespace ${MONITORED_RESOURCE_NAMESPACE}
          node_id ${MONITORED_RESOURCE_NODE_ID}
          log_name_key syslog
      EOF

      # cloudinit_parser
      cat <<EOF >> /etc/td-agent-bit/parsers.conf

      [PARSER]
          Name        cloudinit_parser
          Format      regex
          Regex       ^(?<timestamp>[^ ]+\s[^ ]+) (?<process>[^\[]+)\[(?<pid>\d+)\]:(?<message>.*)
          Time_Key    timestamp
          Time_Format %Y-%m-%d %H:%M:%S

      [PARSER]
          Name        syslog_parser
          Format      regex
          Regex       ^(?<timestamp>\w{3} +\d{1,2} \d{2}:\d{2}:\d{2}) (?<hostname>[^\s]+) (?<identifier>[^:]+): (?<message>.*)
          Time_Key    timestamp
          Time_Format %b %d %H:%M:%S

      [PARSER]
          Name        journald_parser
          Format      regex
          Regex       ^(?<timestamp>\d{4}-\d{2}-\d{2}T[0-9:.+\-Z]+) (?<hostname>[^\s]+) (?<identifier>[^\[]+)\[(?<pid>\d+)\]: (?<message>.*)
          Time_Key    timestamp
          Time_Format %Y-%m-%dT%H:%M:%S.%L%z
      EOF

      systemctl daemon-reload
      systemctl enable td-agent-bit
      systemctl restart td-agent-bit

runcmd:
  - systemctl daemon-reexec
  - systemctl daemon-reload
  - systemctl enable initial-setup.service
  - systemctl start initial-setup.service