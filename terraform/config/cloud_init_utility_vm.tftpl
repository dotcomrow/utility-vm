#cloud-config
hostname: ${hostname}

# Network configuration for static IP
network:
  version: 2
  ethernets:
    eth0:
      dhcp4: false
      addresses:
        - 10.0.0.200/24
      gateway4: 10.0.0.10
      nameservers:
        addresses:
          - 10.0.0.10
          - 8.8.8.8
        search:
          - internal.lan

users:
  - name: ubuntu
    sudo: ["ALL=(ALL) NOPASSWD:ALL"]
    groups: users, admin
    shell: /bin/bash
    ssh_authorized_keys:
      - ${ssh_keys}

ssh_pwauth: false

package_update: true
package_upgrade: true
packages:
  - qemu-guest-agent
  - apt-transport-https
  - ca-certificates
  - curl
  - gnupg-agent
  - software-properties-common
  - zsh
  - nfs-common
  - gnupg
  - lsb-release
  - nftables
  - linux-firmware
  - alsa-utils
  - ubuntu-drivers-common
  - dnsmasq
  - ethtool
  - jq

growpart:
  mode: auto
  devices: ["/"]

write_files:
  - path: /etc/systemd/system/initial-setup.service
    permissions: '0644'
    content: |
      [Unit]
      Description=Post-bootstrap setup for Kubernetes node
      After=network-online.target
      Wants=network-online.target

      [Service]
      Type=oneshot
      EnvironmentFile=-/etc/environment
      ExecStart=/usr/local/bin/initial-setup.sh
      RemainAfterExit=true

      [Install]
      WantedBy=multi-user.target

  - path: /usr/local/bin/initial-setup.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      # export HTTP_PROXY=http://10.0.0.10:3128
      # export HTTPS_PROXY=http://10.0.0.10:3128
      # export NO_PROXY=127.0.0.1,localhost,10.0.0.0/24,10.96.0.0/12

      CPU_FLAGS="nohz_full=0-59 isolcpus=0-59 rcu_nocbs=0-59"
      sed -i "s|^GRUB_CMDLINE_LINUX=.*|GRUB_CMDLINE_LINUX=\"$CPU_FLAGS\"|" /etc/default/grub
      update-grub
      grub-mkconfig -o /boot/grub/grub.cfg || true

      # update-ca-certificates
      systemctl daemon-reload
      /usr/local/bin/performance-tuning.sh
      sysctl --system
      update-grub
      /usr/local/bin/disable-swap.sh
      /usr/local/bin/setup-docker.sh
      /usr/local/bin/setup-fluentbit.sh
      systemctl enable setup-hosts-dns.service
      systemctl start setup-hosts-dns.service
      /usr/local/bin/setup-utility.sh

      systemctl enable qemu-guest-agent
      systemctl start qemu-guest-agent
      systemctl daemon-reload

  - path: /usr/local/bin/setup-utility.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      echo "üîß Setting up utility VM..."

      # Setup dedicated Harbor disk
      echo "üíæ Setting up dedicated Harbor disk..."
      
      # Format the second disk (100GB) for Harbor storage
      HARBOR_DISK="/dev/sdb"
      HARBOR_MOUNT="/data/harbor"
      
      # Check if disk exists
      if [ -b "$HARBOR_DISK" ]; then
        echo "Found Harbor disk: $HARBOR_DISK"
        
        # Create filesystem if not already formatted
        if ! blkid "$HARBOR_DISK"; then
          echo "Formatting Harbor disk with ext4..."
          mkfs.ext4 -F "$HARBOR_DISK"
        else
          echo "Harbor disk already formatted"
        fi
        
        # Create mount point
        mkdir -p "$HARBOR_MOUNT"
        
        # Get UUID for persistent mounting
        DISK_UUID=$(blkid -s UUID -o value "$HARBOR_DISK")
        
        # Add to fstab if not already present
        if ! grep -q "$DISK_UUID" /etc/fstab; then
          echo "UUID=$DISK_UUID $HARBOR_MOUNT ext4 defaults,noatime 0 2" >> /etc/fstab
          echo "Added Harbor disk to fstab"
        fi
        
        # Mount the disk
        mount -a
        
        # Verify mount
        if mountpoint -q "$HARBOR_MOUNT"; then
          echo "‚úÖ Harbor disk mounted successfully at $HARBOR_MOUNT"
          df -h "$HARBOR_MOUNT"
        else
          echo "‚ùå Failed to mount Harbor disk"
          exit 1
        fi
        
        # Set proper ownership and permissions
        chown -R root:root "$HARBOR_MOUNT"
        chmod 755 "$HARBOR_MOUNT"
        
      else
        echo "‚ö†Ô∏è  Harbor disk $HARBOR_DISK not found, using default storage"
        mkdir -p /data/harbor
      fi

      # Harbor installation and configuration
      
      echo "üì¶ Installing Harbor ${HARBOR_VERSION}..."
      
      # Create harbor directories
      mkdir -p /opt/harbor
      mkdir -p /data/harbor
      
      # Download Harbor
      cd /tmp
      wget -q https://github.com/goharbor/harbor/releases/download/${HARBOR_VERSION}/harbor-offline-installer-${HARBOR_VERSION}.tgz
      tar xvf harbor-offline-installer-${HARBOR_VERSION}.tgz
      cp -r harbor/* /opt/harbor/
      
      # Create Harbor configuration
      cd /opt/harbor
      cp harbor.yml.tmpl harbor.yml
      
      # Configure Harbor with SSL disabled
      cat <<EOF > /opt/harbor/harbor.yml
      # Configuration file of Harbor
      
      # The IP address or hostname to access admin UI and registry service.
      hostname: $(hostname).internal.lan
      
      # http related config
      http:
        # port for http, default is 80. If https enabled, this port will redirect to https port
        port: 80
      
      # https related config - DISABLED for pull-through cache
      # https:
      #   port: 443
      #   certificate:
      #   private_key:
      
      # Uncomment external_url if you want to enable external proxy
      # And when it enabled the hostname will no longer used
      external_url: https://harbor.${RANCHER_DOMAIN}
      
      # The initial password of Harbor admin
      harbor_admin_password: Harbor12345
      
      # Authentication configuration
      # Using database authentication initially - GitHub OAuth will be configured via API
      auth_mode: db_auth
      
      # OIDC authentication settings - DISABLED due to GitHub OAuth compatibility issues
      # GitHub OAuth App integration requires manual configuration through Harbor UI
      # oidc_name: GitHub
      # oidc_endpoint: https://github.com/login/oauth/authorize
      # oidc_client_id: ${HARBOR_OIDC_CLIENT}
      # oidc_client_secret: ${HARBOR_OIDC_SECRET}
      # oidc_scope: read:user,read:org
      # oidc_verify_cert: true
      # oidc_auto_onboard: true
      # oidc_user_claim: login
      # oidc_groups_claim: organizations
      # oidc_admin_group: harbor_admins
      # oidc_group_filter: harbor_users,harbor_admins
      
      # Harbor DB configuration
      database:
        # The password for the root user of Harbor DB
        password: root123
        # The maximum number of connections in the idle connection pool (increased for jobservice)
        max_idle_conns: 100
        # The maximum number of open connections to the database (increased for jobservice)
        max_open_conns: 2000
        # Connection timeout for database operations
        conn_max_lifetime: 5m
        # Connection timeout for database connections
        conn_max_idle_time: 10m
      
      # The default data volume
      data_volume: /data/harbor

      # Harbor Storage settings by default is using /data dir on local filesystem
      # Uncomment storage_service setting If you want to using external storage
      # storage_service:
      #   # ca_bundle is the path to the custom root ca certificate, which will be injected into the truststore
      #   # of registry's and chart repository's containers.  This is usually needed when the user hosts a internal storage with self signed certificate.
      #   ca_bundle:
      
      #   # storage backend, default is filesystem, options include filesystem, azure, gcs, s3, swift and oss
      #   # for more info about this configuration please refer https://docs.docker.com/registry/configuration/
      #   filesystem:
      #     maxthreads: 100
      
      # Trivy configuration
      trivy:
        # ignoreUnfixed The flag to display only fixed vulnerabilities
        ignore_unfixed: false
        # skipUpdate The flag to enable or disable Trivy DB downloads from GitHub
        skip_update: false
        # The offline_scan option prevents Trivy from sending API requests to identify dependencies.
        offline_scan: false
        # insecure The flag to skip verifying registry certificate
        insecure: false
      
      jobservice:
        # Maximum number of job workers in job service (increased for better throughput)
        max_job_workers: 20
        # The timeout for webhook job (increased for large image operations)
        webhook_job_timeout: 300
        # Job processing timeout (5 minutes for large operations)
        job_timeout: 300
        # Pool size for job workers
        pool_size: 50
        # Memory settings for jobservice workers
        worker_memory_mb: 512
        # Maximum queue size for pending jobs
        max_queue_size: 10000
        # Job cleanup settings
        cleanup_timeout: 3600
        # Loggers for the job service
        job_loggers:
          - "STD_OUTPUT"
          - "FILE"
          - "DB"
        # Logger sweeper duration (clean logs every 24 hours)
        logger_sweeper_duration: 24
        # Retry settings for failed jobs
        job_retry_count: 3
        job_retry_delay: 10
        # Enable job metrics collection
        enable_metrics: true
      
      notification:
        # Maximum retry count for webhook job
        webhook_job_max_retry: 10
        # HTTP client timeout for webhook jobs (in seconds)
        webhook_job_http_client_timeout: 10
      
      chart:
        # Change the value of absolute_url to enabled can enable absolute url in chart
        absolute_url: disabled
      
      # Log configurations
      log:
        # options are debug, info, warning, error, fatal
        level: info
        # configs for logs in local storage
        local:
          # Log files are rotated log_rotate_count times before being removed. If count is 0, old versions are removed rather than rotated.
          rotate_count: 50
          # Log files are rotated only if they grow bigger than log_rotate_size bytes. If size is followed by k, the size is assumed to be in kilobytes.
          # If the M is used, the size is in megabytes, and if G is used, the size is in gigabytes. So size 100, size 100k, size 100M and size 100G
          # are all valid.
          rotate_size: 200M
          # The directory on your host that store log
          location: /data/harbor/logs
      
      #This attribute is for migrator to detect the version of the .cfg file, DO NOT MODIFY!
      _version: 2.11.0
      
      # Uncomment external_database if using external database.
      # external_database:
      #   harbor:
      #     host: harbor_db_host
      #     port: harbor_db_port
      #     db_name: harbor_db_name
      #     username: harbor_db_username
      #     password: harbor_db_password
      #     ssl_mode: disable
      #     max_idle_conns: 2
      #     max_open_conns: 0
      #   notary_signer:
      #     host: notary_signer_db_host
      #     port: notary_signer_db_port
      #     db_name: notary_signer_db_name
      #     username: notary_signer_db_username
      #     password: notary_signer_db_password
      #     ssl_mode: disable
      #   notary_server:
      #     host: notary_server_db_host
      #     port: notary_server_db_port
      #     db_name: notary_server_db_name
      #     username: notary_server_db_username
      #     password: notary_server_db_password
      #     ssl_mode: disable
      
      # Uncomment external_redis if using external Redis server
      # external_redis:
      #   # support redis, redis+sentinel
      #   # host for redis: <host_redis>:<port_redis>
      #   # host for redis+sentinel:
      #   #  <host_sentinel1>:<port_sentinel1>,<host_sentinel2>:<port_sentinel2>,<host_sentinel3>:<port_sentinel3>
      #   host: redis:6379
      #   password:
      #   # sentinel_master_set must be set to support redis+sentinel
      #   #sentinel_master_set:
      #   # db_index 0 is for core, it's unchangeable
      #   # db_index 1 is for jobservice
      #   # db_index 2 is for trivy
      #   db_index: 0
      
      # Uncomment uaa for trusting the certificate of uaa instance that is hosted via self-signed cert.
      # uaa:
      #   ca_file: /path/to/ca
      
      # Global proxy
      # Config http proxy for components, e.g. http://my.proxy.com:3128
      # Components doesn't need to connect to each others via http proxy.
      # Remove component from `components` array if want disable proxy
      # for it. If you want use proxy for replication, MUST enable proxy
      # for core and jobservice, and set `http_proxy` and `https_proxy`.
      # Add domain to the `no_proxy` list when want disable proxy
      # for some special registry
      proxy:
        http_proxy: ""
        https_proxy: ""
        no_proxy: ""
        components:
          - core
          - jobservice
          - trivy
      
      # metric:
      #   enabled: false
      #   port: 9090
      #   path: /metrics
      
      # Trace related config
      # only can enable one trace at the same time.
      # trace:
      #   enabled: true
      #   # set sample_rate to 1 if you wanna sampling 100% of trace data; set 0.5 if you wanna sampling 50% of trace data, and so forth
      #   sample_rate: 1
      #   # # namespace used to differentiate different harbor services
      #   # namespace:
      #   # # attributes is a key value dict contains user defined attributes used to initialize trace provider
      #   # attributes:
      #   #   application: harbor
      #   # # jaeger should be 1.26 or newer.
      #   # jaeger:
      #   #   endpoint: http://hostname:14268/api/traces
      #   #   username:
      #   #   password:
      #   #   agent_host: hostname
      #   #   agent_port: 6832
      #   # otel:
      #   #   endpoint: hostname:4318
      #   #   url_path: /v1/traces
      #   #   compression: false
      #   #   insecure: true
      #   #   timeout: 10s
      
      # enable purge _upload directories
      upload_purging:
        enabled: true
        # remove files in _upload directories which exist for a period of time, default is one week.
        age: 168h
        # the interval of the purge operations
        interval: 24h
        dryrun: false
      
      # cache layer configurations
      # If this feature enabled, harbor will cache the resource
      # `project/project_metadata/repository/artifact/manifest` in the redis
      # which can especially help to improve the performance of high concurrent
      # manifest pulling.
      # NOTICE
      # If you are deploying Harbor in HA mode, make sure that all the harbor
      # instances have the same behaviour, all with caching enabled or disabled,
      # otherwise it can lead to potential data inconsistency.
      cache:
        # default is not enabled.
        enabled: false
        # default keep cache for one day.
        expire_hours: 24
      EOF
      
      # Create required directories for Harbor logging on dedicated disk
      mkdir -p /data/harbor/logs/jobs
      mkdir -p /data/harbor/logs
      
      # Install Harbor
      echo "üöÄ Installing Harbor..."
      /opt/harbor/install.sh --with-trivy
      
      # Wait for Harbor to start
      echo "‚è≥ Waiting for Harbor to start..."
      sleep 30
      
      # Check and fix jobservice if needed
      echo "üîß Checking Harbor jobservice health..."
      cd /opt/harbor
      
      # Wait for all Harbor services to be ready
      echo "‚è≥ Waiting for Harbor services to be ready..."
      while true; do
        if docker-compose ps | grep -q "Up" && docker-compose ps | grep -q "jobservice"; then
          echo "‚úÖ Harbor services are running"
          break
        fi
        echo "Waiting for Harbor services..."
        sleep 10
      done
      
      # Check jobservice specifically
      JOBSERVICE_STATUS=$(docker ps --filter "name=jobservice" --format "table {{.Status}}" | grep -c "Up" || echo "0")
      if [ "$JOBSERVICE_STATUS" -eq 0 ]; then
        echo "‚ö†Ô∏è  Jobservice not running properly, restarting Harbor..."
        docker-compose down
        sleep 10
        docker-compose up -d
        sleep 30
        
        # Verify jobservice again
        RETRY_STATUS=$(docker ps --filter "name=jobservice" --format "table {{.Status}}" | grep -c "Up" || echo "0")
        if [ "$RETRY_STATUS" -eq 0 ]; then
          echo "‚ùå Jobservice still failing, checking logs..."
          docker-compose logs | grep jobservice
          echo "Attempting jobservice container restart..."
          JOBSERVICE_CONTAINER=$(docker ps -a --filter "name=jobservice" --format "{{.Names}}" | head -1)
          if [ ! -z "$JOBSERVICE_CONTAINER" ]; then
            docker restart "$JOBSERVICE_CONTAINER"
            sleep 15
          else
            echo "‚ùå Could not find jobservice container to restart"
          fi
        else
          echo "‚úÖ Jobservice recovered after restart"
        fi
      else
        echo "‚úÖ Jobservice is running properly"
      fi
      
      # Configure Harbor as pull-through cache
      echo "üîß Configuring Harbor as pull-through cache..."
      
      # Create proxy cache projects for common registries
      HARBOR_HOST="utility-node.internal.lan"
      HARBOR_URL="http://$HARBOR_HOST"
      export HARBOR_USER=${HARBOR_USER}
      export HARBOR_PASS=${HARBOR_PASS}

      
      # Wait for Harbor API to be available with better timing
      echo "‚è≥ Waiting for Harbor API to be fully ready..."
      sleep 60  # Give Harbor more time to fully initialize
      
      until curl -f -s "$HARBOR_URL/api/v2.0/systeminfo" > /dev/null; do
        echo "Waiting for Harbor API to be available..."
        sleep 15
      done
      
      # Wait additional time for Harbor to be fully operational
      echo "‚úÖ Harbor API available, waiting for full initialization..."
      sleep 30
      
      # Function to create proxy cache project
      create_proxy_project() {
        local project_name=$1
        local registry_url=$2
        local registry_name=$3
        
        echo "Creating proxy cache project: $project_name"
        
        # Create registry endpoint first
        echo "Creating registry endpoint: $registry_name"
        REGISTRY_RESPONSE=$(curl -s -w "\n%%{http_code}" -X POST "$HARBOR_URL/api/v2.0/registries" \
          -H "Content-Type: application/json" \
          -u "${HARBOR_USER}:${HARBOR_PASS}" \
          -d "{
            \"name\": \"$registry_name\",
            \"type\": \"docker-registry\",
            \"url\": \"$registry_url\",
            \"insecure\": false,
            \"credential\": {
              \"type\": \"basic\",
              \"access_key\": \"\",
              \"access_secret\": \"\"
            }
          }")
        
        HTTP_CODE=$(echo "$REGISTRY_RESPONSE" | tail -n1)
        RESPONSE_BODY=$(echo "$REGISTRY_RESPONSE" | head -n -1)
        
        if [ "$HTTP_CODE" != "201" ] && [ "$HTTP_CODE" != "409" ]; then
          echo "‚ùå Failed to create registry $registry_name (HTTP $HTTP_CODE): $RESPONSE_BODY"
          return 1
        fi
        
        # Wait a moment for registry to be created
        sleep 5
        
        # Get registry ID
        echo "Getting registry ID for: $registry_name"
        REGISTRY_ID=""
        while true; do
          REGISTRY_ID=$(curl -s "$HARBOR_URL/api/v2.0/registries" \
            -u "${HARBOR_USER}:${HARBOR_PASS}" | \
            jq -r ".[] | select(.name==\"$registry_name\") | .id" 2>/dev/null)
          
          if [ ! -z "$REGISTRY_ID" ] && [ "$REGISTRY_ID" != "null" ]; then
            echo "‚úÖ Found registry ID: $REGISTRY_ID"
            break
          fi
          echo "Waiting for registry to be available..."
          sleep 3
        done
        
        if [ -z "$REGISTRY_ID" ] || [ "$REGISTRY_ID" == "null" ]; then
          echo "‚ùå Failed to get registry ID for $registry_name"
          return 1
        fi
        
        # Create proxy cache project
        echo "Creating proxy cache project: $project_name with registry ID: $REGISTRY_ID"
        PROJECT_RESPONSE=$(curl -s -w "\n%%{http_code}" -X POST "$HARBOR_URL/api/v2.0/projects" \
          -H "Content-Type: application/json" \
          -u "${HARBOR_USER}:${HARBOR_PASS}" \
          -d "{
            \"project_name\": \"$project_name\",
            \"registry_id\": $REGISTRY_ID,
            \"metadata\": {
              \"public\": \"true\"
            }
          }")
        
        PROJECT_HTTP_CODE=$(echo "$PROJECT_RESPONSE" | tail -n1)
        PROJECT_RESPONSE_BODY=$(echo "$PROJECT_RESPONSE" | head -n -1)
        
        if [ "$PROJECT_HTTP_CODE" != "201" ] && [ "$PROJECT_HTTP_CODE" != "409" ]; then
          echo "‚ùå Failed to create project $project_name (HTTP $PROJECT_HTTP_CODE): $PROJECT_RESPONSE_BODY"
          return 1
        fi
        
        echo "‚úÖ Successfully created proxy cache project: $project_name"
        sleep 2  # Brief pause between projects
      }
      
      # Create proxy cache projects for popular registries
      create_proxy_project "dockerhub" "https://registry-1.docker.io" "docker-hub"
      create_proxy_project "gcr" "https://gcr.io" "google-gcr"
      create_proxy_project "quay" "https://quay.io" "quay-io"
      create_proxy_project "k8s-gcr" "https://k8s.gcr.io" "k8s-gcr-io"
      create_proxy_project "registry-k8s" "https://registry.k8s.io" "registry-k8s-io"
      
      echo "‚úÖ Harbor installation completed!"
      echo "üåê Harbor is available at: http://$HARBOR_HOST"
      echo "üåê External Harbor URL: https://harbor.${RANCHER_DOMAIN}"
      echo "üîê Harbor authentication: Database mode (admin / Harbor12345)"
      echo "‚ö†Ô∏è  IMPORTANT: To enable GitHub OAuth:"
      echo "   1. Deploy Dex OIDC provider as intermediary"
      echo "   2. Configure Dex with GitHub OAuth credentials"
      echo "   3. Configure Harbor to trust Dex as OIDC provider"
      echo "   Note: Harbor does not support direct GitHub OAuth - requires OIDC provider like Dex"
      echo ""
      echo "üîß Use 'harbor-manage' command to manage Harbor service"
      echo ""
      echo "üìã Proxy cache projects created:"
      echo "  - dockerhub (Docker Hub): $HARBOR_HOST/dockerhub/"
      echo "  - gcr (Google Container Registry): $HARBOR_HOST/gcr/"
      echo "  - quay (Quay.io): $HARBOR_HOST/quay/"
      echo "  - k8s-gcr (Kubernetes GCR): $HARBOR_HOST/k8s-gcr/"
      echo "  - registry-k8s (Kubernetes Registry): $HARBOR_HOST/registry-k8s/"
      echo ""
      echo "üöÄ Configure your Kubernetes cluster to use Harbor as pull-through cache:"
      echo "  Example: docker pull $HARBOR_HOST/dockerhub/nginx:latest"
      echo ""
      
      # Deploy Dex OIDC provider if GitHub OAuth credentials are provided
      if [ -n "${HARBOR_OIDC_CLIENT}" ] && [ -n "${HARBOR_OIDC_SECRET}" ]; then
        echo "üîß Deploying Dex OIDC provider for GitHub OAuth integration..."
        /usr/local/bin/setup-dex.sh
        
        # Configure Harbor to use Dex OIDC
        echo "üîß Configuring Harbor to use Dex OIDC..."
        sleep 10  # Wait for Dex to be fully ready
        
        # Get the Dex client secret that was generated
        DEX_CLIENT_SECRET=$(grep "secret:" /opt/dex/config.yaml | sed "s/.*secret: '//" | sed "s/'.*//")
        
        # Configure Harbor OIDC settings
        OIDC_CONFIG_RESPONSE=$(curl -s -w "\n%%{http_code}" -X PUT "$HARBOR_URL/api/v2.0/configurations" \
          -H "Content-Type: application/json" \
          -u "${HARBOR_USER}:${HARBOR_PASS}" \
          -d "{
            \"auth_mode\": \"oidc_auth\",
            \"oidc_name\": \"GitHub via Dex\",
            \"oidc_endpoint\": \"https://dex.${RANCHER_DOMAIN}\",
            \"oidc_client_id\": \"harbor\",
            \"oidc_client_secret\": \"$DEX_CLIENT_SECRET\",
            \"oidc_scope\": \"openid email profile groups\",
            \"oidc_verify_cert\": false,
            \"oidc_auto_onboard\": true,
            \"oidc_user_claim\": \"email\",
            \"oidc_groups_claim\": \"groups\"
          }")
        
        OIDC_HTTP_CODE=$(echo "$OIDC_CONFIG_RESPONSE" | tail -n1)
        OIDC_RESPONSE_BODY=$(echo "$OIDC_CONFIG_RESPONSE" | head -n -1)
        
        if [ "$OIDC_HTTP_CODE" = "200" ]; then
          echo "‚úÖ Harbor configured to use Dex OIDC for GitHub authentication"
          echo "üîó GitHub OIDC Login URL: https://harbor.${RANCHER_DOMAIN}/c/oidc/login"
          echo "üë§ Users can now login with their GitHub accounts"
        else
          echo "‚ö†Ô∏è  Harbor OIDC configuration failed (HTTP $OIDC_HTTP_CODE)"
          echo "Response: $OIDC_RESPONSE_BODY"
          echo "Harbor will continue using database authentication"
        fi
        
        echo "‚úÖ GitHub OAuth authentication via Dex configured successfully"
      else
        echo "‚ÑπÔ∏è  No GitHub OAuth credentials provided"
        echo "‚ÑπÔ∏è  Harbor using database authentication (admin/Harbor12345)"
        echo "‚ÑπÔ∏è  To enable GitHub OAuth, provide HARBOR_OIDC_CLIENT and HARBOR_OIDC_SECRET variables"
      fi
      
      # Create comprehensive Harbor management script
      cat > /usr/local/bin/harbor-manage << 'HARBORMGMTEOF'
      #!/bin/bash
      cd /opt/harbor
      case "$1" in
        start)
          docker-compose up -d
          ;;
        stop)
          docker-compose down
          ;;
        restart)
          docker-compose down
          sleep 5
          docker-compose up -d
          ;;
        status)
          docker-compose ps
          ;;
        logs)
          docker-compose logs -f
          ;;
        auth-status)
          echo "Harbor Authentication Status:"
          HARBOR_URL="http://utility-node.internal.lan"
          AUTH_MODE=$(curl -s "$HARBOR_URL/api/v2.0/configurations" -u "admin:Harbor12345" | jq -r '.auth_mode.value // .auth_mode' 2>/dev/null)
          if [ "$AUTH_MODE" = "oidc_auth" ]; then
            echo "‚úÖ OIDC Authentication enabled"
            OIDC_NAME=$(curl -s "$HARBOR_URL/api/v2.0/configurations" -u "admin:Harbor12345" | jq -r '.oidc_name.value // .oidc_name' 2>/dev/null)
            OIDC_ENDPOINT=$(curl -s "$HARBOR_URL/api/v2.0/configurations" -u "admin:Harbor12345" | jq -r '.oidc_endpoint.value // .oidc_endpoint' 2>/dev/null)
            echo "   Provider: $OIDC_NAME"
            echo "   Endpoint: $OIDC_ENDPOINT"
          else
            echo "‚ÑπÔ∏è  Database Authentication enabled (auth_mode: $AUTH_MODE)"
          fi
          ;;
        oidc-login-url)
          echo "GitHub OIDC Login URL: https://harbor.${RANCHER_DOMAIN}/c/oidc/login"
          ;;
        health)
          HARBOR_URL="http://utility-node.internal.lan"
          if curl -f -s "$HARBOR_URL/api/v2.0/systeminfo" >/dev/null; then
            echo "‚úÖ Harbor API is healthy"
          else
            echo "‚ùå Harbor API is not responding"
          fi
          ;;
        jobservice-restart)
          echo "Restarting Harbor jobservice..."
          JOBSERVICE_CONTAINER=$(docker ps -a --filter "name=jobservice" --format "{{.Names}}" | head -1)
          if [ ! -z "$JOBSERVICE_CONTAINER" ]; then
            docker restart "$JOBSERVICE_CONTAINER"
            sleep 10
            docker ps --filter "name=jobservice"
          else
            echo "‚ùå Could not find jobservice container"
          fi
          ;;
        jobservice-logs)
          echo "Harbor jobservice logs:"
          JOBSERVICE_CONTAINER=$(docker ps -a --filter "name=jobservice" --format "{{.Names}}" | head -1)
          if [ ! -z "$JOBSERVICE_CONTAINER" ]; then
            docker logs -f "$JOBSERVICE_CONTAINER"
          else
            echo "‚ùå Could not find jobservice container"
          fi
          ;;
        *)
          echo "Usage: $0 {start|stop|restart|status|logs|auth-status|oidc-login-url|health|jobservice-restart|jobservice-logs}"
          exit 1
          ;;
      esac
      HARBORMGMTEOF
      
      chmod +x /usr/local/bin/harbor-manage
    else
      echo "‚ùå Harbor installation failed!" | tee -a /var/log/harbor-install.log
      exit 1  
    fi

  - path: /usr/local/bin/setup-dex.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      set -e
      echo "üîß Installing and configuring Dex OIDC provider..."
      
      # Create directories
      mkdir -p /opt/dex /data/dex
      
      # Download and install Dex
      cd /tmp
      DEX_VERSION="v2.37.0"
      echo "üì¶ Downloading Dex $DEX_VERSION..."
      wget -q "https://github.com/dexidp/dex/releases/download/$DEX_VERSION/dex-$DEX_VERSION-linux-amd64.tar.gz"
      tar xzf "dex-$DEX_VERSION-linux-amd64.tar.gz"
      cp "dex-$DEX_VERSION-linux-amd64/dex" /usr/local/bin/dex
      chmod +x /usr/local/bin/dex
      
      # Generate client secret for Harbor
      DEX_CLIENT_SECRET=$(openssl rand -hex 16)
      echo "Generated Dex client secret for Harbor"
      
      # Create Dex configuration
      cat > /opt/dex/config.yaml << 'DEXEOF'
      issuer: https://dex.${RANCHER_DOMAIN}
      
      storage:
        type: sqlite3
        config:
          file: /data/dex/dex.db
      
      web:
        http: 0.0.0.0:5556
        # tlsCert: /etc/dex/tls/tls.crt
        # tlsKey: /etc/dex/tls/tls.key
      
      connectors:
      - type: github
        id: github
        name: GitHub
        config:
          clientID: ${HARBOR_OIDC_CLIENT}
          clientSecret: ${HARBOR_OIDC_SECRET}
          redirectURI: https://dex.${RANCHER_DOMAIN}/callback
          orgs:
          - name: ${GITHUB_ORG:-dotcomrow}
          teamNameField: slug
          useLoginAsID: false
      
      oauth2:
        skipApprovalScreen: true
        alwaysShowLoginScreen: false
      
      staticClients:
      - id: harbor
        redirectURIs:
        - 'https://harbor.${RANCHER_DOMAIN}/c/oidc/callback'
        name: 'Harbor Registry'
        secret: 'DEX_CLIENT_SECRET_PLACEHOLDER'
      
      enablePasswordDB: false
      
      logger:
        level: info
        format: json
      DEXEOF
      
      # Replace placeholder with actual secret
      sed -i "s/DEX_CLIENT_SECRET_PLACEHOLDER/$DEX_CLIENT_SECRET/" /opt/dex/config.yaml
      
      # Create dex user
      useradd --system --shell /bin/false --home-dir /data/dex --create-home dex 2>/dev/null || true
      chown -R dex:dex /data/dex /opt/dex
      
      # Create systemd service
      cat > /etc/systemd/system/dex.service << 'DEXSVCEOF'
      [Unit]
      Description=Dex OIDC Identity Provider
      Documentation=https://dexidp.io/docs/
      After=network-online.target
      Wants=network-online.target
      
      [Service]
      Type=simple
      User=dex
      Group=dex
      ExecStart=/usr/local/bin/dex serve /opt/dex/config.yaml
      Restart=on-failure
      RestartSec=5
      TimeoutStopSec=30
      NoNewPrivileges=true
      PrivateTmp=true
      ProtectSystem=strict
      ProtectHome=true
      ReadWritePaths=/data/dex
      CapabilityBoundingSet=CAP_NET_BIND_SERVICE
      AmbientCapabilities=CAP_NET_BIND_SERVICE
      
      [Install]
      WantedBy=multi-user.target
      DEXSVCEOF
      
      # Create Dex management script
      cat > /usr/local/bin/dex-manage << 'DEXMGMTEOF'
      #!/bin/bash
      case "$1" in
        start)
          systemctl start dex
          ;;
        stop)
          systemctl stop dex
          ;;
        restart)
          systemctl restart dex
          ;;
        status)
          systemctl status dex
          ;;
        logs)
          journalctl -u dex -f
          ;;
        config)
          cat /opt/dex/config.yaml
          ;;
        health)
          curl -f http://localhost:5556/dex/healthz || echo "Dex health check failed"
          ;;
        *)
          echo "Usage: $0 {start|stop|restart|status|logs|config|health}"
          exit 1
          ;;
      esac
      DEXMGMTEOF
      
      chmod +x /usr/local/bin/dex-manage
      
      # Start and enable Dex service
      systemctl daemon-reload
      systemctl enable dex
      systemctl start dex
      
      # Wait for Dex to start and verify
      echo "‚è≥ Waiting for Dex to start..."
      sleep 10
      
      # Health check
      for i in {1..30}; do
        if systemctl is-active --quiet dex && curl -f -s http://localhost:5556/dex/healthz >/dev/null 2>&1; then
          echo "‚úÖ Dex OIDC provider started successfully"
          echo "üåê Dex issuer: https://dex.${RANCHER_DOMAIN}"
          echo "üîß Use 'dex-manage' command to manage Dex service"
          exit 0
        fi
        echo "Waiting for Dex to be ready... ($i/30)"
        sleep 2
      done
      
      echo "‚ùå Dex failed to start properly"
      echo "Service status:"
      systemctl status dex
      echo "Logs:"
      journalctl -u dex --lines=20
      exit 1

  - path: /usr/local/bin/performance-tuning.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      echo "üîß Applying performance tuning..."
      
      # CPU Governor
      GOVERNOR_PATH="/sys/devices/system/cpu/cpu0/cpufreq/scaling_governor"
      if [ -f "$GOVERNOR_PATH" ]; then
        for CPU in /sys/devices/system/cpu/cpu[0-9]*; do
          echo performance > "$CPU/cpufreq/scaling_governor" 2>/dev/null || true
        done
        echo "‚úÖ CPU governors set to performance"
      else
        echo "‚ö†Ô∏è CPU frequency scaling not available ‚Äî skipping governor tuning"
      fi
      
      # Scheduler tuning for SSDs
      for dev in /sys/block/sd*/queue/scheduler; do
        echo mq-deadline > "$dev"
      done

      systemctl disable apt-daily.timer
      systemctl disable snapd
      systemctl disable motd-news.timer
      
      # Sysctl parameters
      cat <<EOF > /etc/sysctl.d/99-performance-tuning.conf
      fs.inotify.max_user_watches=524288
      vm.max_map_count=262144
      net.core.rmem_max=134217728
      net.core.wmem_max=134217728
      net.ipv4.tcp_rmem=4096 87380 134217728
      net.ipv4.tcp_wmem=4096 65536 134217728
      net.ipv4.tcp_congestion_control=bbr
      vm.swappiness=10
      vm.dirty_ratio=15
      vm.dirty_background_ratio=5
      EOF
      
      sysctl --system

      # 2) Pick all ‚Äúlarge-MTU‚Äù interfaces automatically
      #    (adjust the grep if your jumbo-MTU devices have a different naming scheme)
      for IF in $(ip -o link show | awk -F': ' '{print $2}' | grep -E '^(eth|vtnet)'); do
        # bring MTU up before offloads
        ip link set dev $IF mtu 9000

        # enable big-packet offloads
        ethtool -K $IF gro on gso on tso on rx on tx on

        # increase the kernel TX queue length
        ip link set dev $IF txqueuelen 10000
      done

      # 3) Sysctls ‚Äî bump backlog and buffers for all interfaces
      #    and enable TCP MTU probing (helps path-MTU discovery)
      sysctl -w net.core.netdev_max_backlog=250000
      sysctl -w net.core.rmem_max=134217728
      sysctl -w net.core.wmem_max=134217728
      sysctl -w net.ipv4.tcp_rmem="4096 87380 134217728"
      sysctl -w net.ipv4.tcp_wmem="4096 87380 134217728"
      sysctl -w net.ipv4.tcp_mtu_probing=1

      # 4) Persist sysctls
      cat <<EOF >/etc/sysctl.d/99-network-opt.conf
      net.core.netdev_max_backlog=250000
      net.core.rmem_max=134217728
      net.core.wmem_max=134217728
      net.ipv4.tcp_rmem=4096 87380 134217728
      net.ipv4.tcp_wmem=4096 87380 134217728
      net.ipv4.tcp_mtu_probing=1
      EOF

      # 5) Persist ethtool + txqueuelen via a systemd service
      cat <<'EOF' >/etc/systemd/system/network-tweaks.service
      [Unit]
      Description=Apply ethtool offloads + txqueuelen
      After=network-online.target
      Wants=network-online.target

      [Service]
      Type=oneshot
      ExecStart=/usr/local/bin/network-tweaks.sh

      [Install]
      WantedBy=multi-user.target
      EOF

      # Install the same script to /usr/local/bin and make it executable
            cat <<'EOF' >/usr/local/bin/network-tweaks.sh
      #!/bin/bash
      set -e
      for IF in $(ip -o link show | awk -F': ' '{print $2}' | grep -E '^(eth|vtnet)'); do
        CURRENT_MTU=$(ip -o link show "$IF" | awk '{print $4}')
        if [ "$CURRENT_MTU" != "9000" ]; then
          ip link set dev "$IF" mtu 9000
        fi
        ethtool -K $IF gro on gso on tso on rx on tx on
        ip link set dev $IF txqueuelen 10000
      done
      EOF

      chmod +x /usr/local/bin/network-tweaks.sh

      systemctl daemon-reload
      systemctl enable network-tweaks

      echo "‚úÖ Network tweaks applied and will persist across reboots."
      
      # Limit nofile
      cat <<EOF > /etc/security/limits.d/99-nofile.conf
      * soft nofile 1048576
      * hard nofile 1048576
      EOF
      
      # Systemd service overrides
      mkdir -p /etc/systemd/system/containerd.service.d
      mkdir -p /etc/systemd/system/rke2-server.service.d
      mkdir -p /etc/systemd/system/rke2-agent.service.d
      
      cat <<EOF > /etc/systemd/system/containerd.service.d/override.conf
      [Service]
      LimitNOFILE=1048576
      LimitNPROC=65536
      TasksMax=infinity
      EOF
      
      cat <<EOF > /etc/systemd/system/rke2-server.service.d/override.conf
      [Service]
      LimitNOFILE=1048576
      LimitNPROC=65536
      TasksMax=infinity
      EOF
      
      cat <<EOF > /etc/systemd/system/rke2-agent.service.d/override.conf
      [Service]
      LimitNOFILE=1048576
      LimitNPROC=65536
      TasksMax=infinity
      EOF

      # cat <<EOF | sudo tee /etc/profile.d/http_proxy.sh
      # export HTTP_PROXY="http://10.0.0.10:3128"
      # export HTTPS_PROXY="http://10.0.0.10:3128"
      # export NO_PROXY="127.0.0.1,localhost,10.0.0.0/24,10.96.0.0/12,192.168.0.0/16"
      # EOF

      # sudo chmod +x /etc/profile.d/http_proxy.sh

      # Unbind kernel workqueues from specific CPUs (recommended on NUMA/multicore systems)
      if [ -e /sys/module/workqueue/parameters/disable_bound ]; then
        echo y > /sys/module/workqueue/parameters/disable_bound
        echo "‚úÖ Enabled unbound kernel workqueues for better scalability"
      fi

      # 6) Add GRUB performance kernel parameters
      echo "‚úÖ Adding performance kernel parameters to GRUB..."
      GRUB_CONFIG_FILE="/etc/default/grub"

      # Append only if not already present
      if ! grep -q "workqueue.disable_bound=1" "$GRUB_CONFIG_FILE"; then
        sed -i 's/GRUB_CMDLINE_LINUX_DEFAULT="/GRUB_CMDLINE_LINUX_DEFAULT="workqueue.disable_bound=1 intel_pstate=disable processor.max_cstate=1 idle=poll transparent_hugepage=never mitigations=off /' "$GRUB_CONFIG_FILE"
        update-grub
        echo "‚úÖ GRUB updated. Kernel parameters will apply on next boot."
      else
        echo "‚ö†Ô∏è GRUB already configured. Skipping update."
      fi
      
      systemctl daemon-reload

  - path: /usr/local/bin/setup-hosts-dns.sh
    permissions: '0755'
    content: |
      #!/bin/bash

      # Add hostname to /etc/hosts without modifying network config
      HOSTNAME=$(hostname)
      grep -q "127.0.1.1 $HOSTNAME" /etc/hosts || echo "127.0.1.1 $HOSTNAME" >> /etc/hosts

      # ---------------------------------------------
      # üßπ Disable systemd-resolved to free port 53
      # ---------------------------------------------
      echo "üßπ Disabling systemd-resolved..."
      sudo systemctl stop systemd-resolved || true
      sudo systemctl disable systemd-resolved || true
      sudo rm -f /etc/resolv.conf
      echo "nameserver 127.0.0.1" | sudo tee /etc/resolv.conf > /dev/null

      # ---------------------------------------------
      # ‚öôÔ∏è Configure dnsmasq split-DNS
      # ---------------------------------------------
      sudo tee /etc/dnsmasq.d/k8s-split-dns.conf > /dev/null <<EOF
      # Forward Kubernetes service lookups to CoreDNS
      server=/svc.cluster.local/10.43.0.10

      # Forward internal LAN lookups to LAN DNS
      server=/internal.lan/10.0.0.10

      # Forward everything else to LAN DNS
      server=10.0.0.10

      # Listen locally
      listen-address=127.0.0.1

      # Don't use /etc/resolv.conf
      no-resolv
      EOF

      # Ensure dnsmasq global config allows localhost
      sudo sed -i 's/^#\?listen-address=.*/listen-address=127.0.0.1/' /etc/dnsmasq.conf || echo 'listen-address=127.0.0.1' | sudo tee -a /etc/dnsmasq.conf

      sudo systemctl enable dnsmasq
      sudo systemctl restart dnsmasq

      # Confirm DNS server
      echo "‚úÖ dnsmasq is resolving:"
      echo "  - *.svc.cluster.local ‚Üí 10.43.0.10"
      echo "  - *.internal.lan      ‚Üí 10.0.0.10"
      echo "  - all others          ‚Üí 10.0.0.10"

      # ---------------------------------------------
      # üõ° IPv6 Hard Disable (Optional)
      # ---------------------------------------------
      sudo modprobe -r ip6_tables || true
      sudo modprobe -r nf_defrag_ipv6 || true
      sudo modprobe -r nf_conntrack_ipv6 || true
      sudo modprobe -r ipv6 || true

      echo "install ipv6 /bin/true" | sudo tee /etc/modprobe.d/force-disable-ipv6.conf
      echo "install ip6_tables /bin/true" | sudo tee -a /etc/modprobe.d/force-disable-ipv6.conf

      if [ -e /usr/sbin/ip6tables ]; then
        sudo mv /usr/sbin/ip6tables /usr/sbin/ip6tables.disabled
      fi

      sudo tee /etc/modprobe.d/disable-ipv6.conf > /dev/null <<EOF
      blacklist ipv6
      blacklist nf_conntrack_ipv6
      blacklist nf_defrag_ipv6
      blacklist ip6_tables
      EOF

      sudo update-initramfs -u

      # ---------------------------------------------
      # üîÅ Force sysctl re-apply on boot
      # ---------------------------------------------
      sudo tee /etc/systemd/system/sysctl-ensure.service > /dev/null <<EOF
      [Unit]
      Description=Force re-apply sysctl settings
      After=network.target
      Wants=network.target

      [Service]
      Type=oneshot
      ExecStart=/sbin/sysctl --system

      [Install]
      WantedBy=multi-user.target
      EOF

      sudo systemctl daemon-reexec
      sudo systemctl enable sysctl-ensure.service

  - path: /etc/systemd/system/setup-hosts-dns.service
    permissions: '0644'
    content: |
      [Unit]
      Description=Setup hosts and DNS configuration
      After=network-online.target
      Wants=network-online.target

      [Service]
      Type=oneshot
      ExecStart=/usr/local/bin/setup-hosts-dns.sh
      RemainAfterExit=true

      [Install]
      WantedBy=multi-user.target

  - path: /etc/sysctl.d/99-disable-ipv6.conf
    permissions: '0644'
    content: |
      net.ipv6.conf.all.disable_ipv6 = 1
      net.ipv6.conf.default.disable_ipv6 = 1
      net.ipv6.conf.lo.disable_ipv6 = 1

  - path: /etc/default/grub.d/99-disable-ipv6.cfg
    permissions: '0644'
    content: |
      GRUB_CMDLINE_LINUX="$GRUB_CMDLINE_LINUX ipv6.disable=1"

  - path: /usr/local/bin/disable-swap.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      swapoff -a
      cp /etc/fstab /etc/fstab.bak
      sed -i '/^[^#].*\bswap\b/s/^/#/' /etc/fstab
      echo fs.inotify.max_user_watches=524288 | sudo tee /etc/sysctl.d/99-inotify.conf

  - path: /usr/local/bin/setup-docker.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      until curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -; do sleep 5; done
      sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
      sudo apt-get update -y
      DEBIAN_FRONTEND=noninteractive sudo apt-get install -y docker-ce docker-ce-cli containerd.io

      # Install docker-compose immediately after Docker
      echo "üì¶ Installing docker-compose..."
      curl -L "https://github.com/docker/compose/releases/download/v2.24.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
      chmod +x /usr/local/bin/docker-compose
      
      # Verify docker-compose installation
      docker-compose --version
      echo "‚úÖ Docker and docker-compose installed successfully"

      # sudo mkdir -p /etc/systemd/system/docker.service.d
      # cat <<EOF | sudo tee /etc/systemd/system/docker.service.d/10-http-proxy.conf
      # [Service]
      # Environment="HTTP_PROXY=http://10.0.0.10:3128" "HTTPS_PROXY=http://10.0.0.10:3128" "NO_PROXY=127.0.0.1,localhost,10.0.0.0/24,10.96.0.0/12"
      # EOF

      # sudo mkdir -p /etc/systemd/system/containerd.service.d
      # cat <<EOF | sudo tee /etc/systemd/system/containerd.service.d/10-http-proxy.conf
      # [Service]
      # Environment="HTTP_PROXY=http://10.0.0.10:3128" "HTTPS_PROXY=http://10.0.0.10:3128" "NO_PROXY=127.0.0.1,localhost,10.0.0.0/24,10.96.0.0/12"
      # EOF

  - path: /etc/security/limits.d/99-nofile.conf
    permissions: '0644'
    content: |
      * soft nofile 1048576
      * hard nofile 1048576

  - path: /etc/systemd/system/containerd.service.d/override.conf
    permissions: '0644'
    content: |
      [Service]
      LimitNOFILE=1048576
      LimitNPROC=65536
      TasksMax=infinity

  - path: /etc/systemd/system/rke2-server.service.d/override.conf
    permissions: '0644'
    content: |
      [Service]
      LimitNOFILE=1048576
      LimitNPROC=65536
      TasksMax=infinity

  - path: /etc/systemd/system/rke2-agent.service.d/override.conf
    permissions: '0644'
    content: |
      [Service]
      LimitNOFILE=1048576
      LimitNPROC=65536
      TasksMax=infinity

  - path: /usr/local/bin/setup-fluentbit.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      set -e
      echo "üì¶ Configuring Fluent Bit for GCP logging..."

      mkdir -p /etc/google-cloud-ops-agent
      echo "${GCP_LOGGING_KEY}" | base64 -d > /etc/google-cloud-ops-agent/logging-key.json

      mkdir -p /usr/share/keyrings
      KEYRING_PATH="/usr/share/keyrings/fluent-bit-archive-keyring.gpg"

      # Clean up any broken or partial key
      rm -f "$KEYRING_PATH"

      until curl -fsSL https://packages.fluentbit.io/fluentbit.key | gpg --dearmor --no-tty --batch -o "$KEYRING_PATH"; do
        echo "Waiting for DNS resolution or valid key for fluentbit.key..."
        sleep 5
        rm -f "$KEYRING_PATH"  # ensure no corrupted key is left behind
      done

      DISTRO=$(lsb_release -cs)
      if [ "$DISTRO" = "${UBUNTU_RELEASE_CODE_NAME}" ]; then
        echo "Detected codename $DISTRO unsupported, falling back to jammy"
        DISTRO="jammy"
      fi

      echo "deb [signed-by=/usr/share/keyrings/fluent-bit-archive-keyring.gpg] https://packages.fluentbit.io/ubuntu/$DISTRO $DISTRO main" > /etc/apt/sources.list.d/fluent-bit.list
      sudo apt-get update
      sudo apt-get install -y td-agent-bit

      mkdir -p /etc/td-agent-bit/conf.d

      cat <<EOF > /etc/td-agent-bit/td-agent-bit.conf
      [SERVICE]
          flush         1
          daemon        Off
          log_level     info
          plugins_file  plugins.conf
          parsers_file  parsers.conf
          http_server   Off
          http_listen   0.0.0.0
          http_port     2020
          storage.metrics on

      @INCLUDE /etc/td-agent-bit/conf.d/*.conf
      EOF

      # cloud-init logs
      cat <<EOF > /etc/td-agent-bit/conf.d/cloudinit.conf
      [INPUT]
          Name               tail
          Mem_Buf_Limit     10MB
          Path               /var/log/cloud-init*.log
          Tag                cloudinit
          Read_from_Head     Off
          DB                 /var/log/flb_cloudinit.db
          DB.Sync            normal
          Skip_Long_Lines    On

      [FILTER]
          Name      parser
          Match     cloudinit
          Key_Name  log
          Parser    cloudinit_parser
          Reserve_Data true
          Preserve_Key true

      [FILTER]
          Name modify
          Match cloudinit
          Add logName cloudinit

      [FILTER]
          Name    nest
          Match   cloudinit
          Operation nest
          Wildcard process
          Wildcard pid
          Wildcard timestamp
          Nested_under logging.googleapis.com/labels

      [OUTPUT]
          Name  stackdriver
          Match cloudinit
          google_service_credentials /etc/google-cloud-ops-agent/logging-key.json
          resource ${MONITORED_RESOURCE_TYPE}
          location ${MONITORED_RESOURCE_LOCATION}
          namespace ${MONITORED_RESOURCE_NAMESPACE}
          node_id ${MONITORED_RESOURCE_NODE_ID}
          log_name_key cloudinit
      EOF

      # journal logs (systemd, rke2, etc.)
      cat <<EOF > /etc/td-agent-bit/conf.d/journald.conf
      [INPUT]
          Name    systemd
          Tag     journal.*
          Mem_Buf_Limit     10MB
          DB      /var/log/flb_journal.db
          Systemd_Filter  _SYSTEMD_UNIT=bootstrap-rke2.service
          Systemd_Filter  _SYSTEMD_UNIT=rke2-server.service
          Systemd_Filter  _SYSTEMD_UNIT=cloud-final.service
          Read_From_Tail  On

      [FILTER]
          Name      parser
          Match     journal.*
          Key_Name  MESSAGE
          Parser    journald_parser
          Reserve_Data true
          Preserve_Key true

      [FILTER]
          Name modify
          Match journal.*
          Add logName systemd-journal

      [FILTER]
          Name   modify
          Match  journal.*
          Rename message textPayload

      [FILTER]
          Name    nest
          Match   journal.*
          Operation nest
          Wildcard identifier
          Wildcard pid
          Wildcard hostname
          Nested_under logging.googleapis.com/labels

      [OUTPUT]
          Name  stackdriver
          Match journal.*
          google_service_credentials /etc/google-cloud-ops-agent/logging-key.json
          resource ${MONITORED_RESOURCE_TYPE}
          location ${MONITORED_RESOURCE_LOCATION}
          namespace ${MONITORED_RESOURCE_NAMESPACE}
          node_id ${MONITORED_RESOURCE_NODE_ID}
          log_name_key systemd-journal
      EOF

      # Optional: catch all syslog (if rsyslog is enabled)
      cat <<EOF > /etc/td-agent-bit/conf.d/syslog.conf
      [INPUT]
          Name              tail
          Mem_Buf_Limit     10MB
          Path              /var/log/syslog
          Tag               syslog
          Read_from_Head    Off
          DB                /var/log/flb_syslog.db
          Skip_Long_Lines   On

      [FILTER]
          Name   parser
          Match  syslog
          Key_Name log
          Parser syslog_parser
          Reserve_Data true
          Preserve_Key true

      [FILTER]
          Name   modify
          Match  syslog
          Rename message textPayload

      [FILTER]
          Name modify
          Match syslog
          Add logName syslog

      [FILTER]
          Name    nest
          Match   syslog
          Operation nest
          Wildcard hostname
          Wildcard identifier
          Wildcard pid
          Nested_under logging.googleapis.com/labels

      [OUTPUT]
          Name  stackdriver
          Match syslog
          google_service_credentials /etc/google-cloud-ops-agent/logging-key.json
          resource ${MONITORED_RESOURCE_TYPE}
          location ${MONITORED_RESOURCE_LOCATION}
          namespace ${MONITORED_RESOURCE_NAMESPACE}
          node_id ${MONITORED_RESOURCE_NODE_ID}
          log_name_key syslog
      EOF

      # cloudinit_parser
      cat <<EOF >> /etc/td-agent-bit/parsers.conf

      [PARSER]
          Name        cloudinit_parser
          Format      regex
          Regex       ^(?<timestamp>[^ ]+\s[^ ]+) (?<process>[^\[]+)\[(?<pid>\d+)\]:(?<message>.*)
          Time_Key    timestamp
          Time_Format %Y-%m-%d %H:%M:%S

      [PARSER]
          Name        syslog_parser
          Format      regex
          Regex       ^(?<timestamp>\w{3} +\d{1,2} \d{2}:\d{2}:\d{2}) (?<hostname>[^\s]+) (?<identifier>[^:]+): (?<message>.*)
          Time_Key    timestamp
          Time_Format %b %d %H:%M:%S

      [PARSER]
          Name        journald_parser
          Format      regex
          Regex       ^(?<timestamp>\d{4}-\d{2}-\d{2}T[0-9:.+\-Z]+) (?<hostname>[^\s]+) (?<identifier>[^\[]+)\[(?<pid>\d+)\]: (?<message>.*)
          Time_Key    timestamp
          Time_Format %Y-%m-%dT%H:%M:%S.%L%z
      EOF

      systemctl daemon-reload
      systemctl enable td-agent-bit
      systemctl restart td-agent-bit

runcmd:
  - systemctl daemon-reexec
  - systemctl daemon-reload
  - systemctl enable initial-setup.service
  - systemctl start initial-setup.service