#cloud-config
hostname: ${hostname}

# Network configuration for static IP
network:
  version: 2
  ethernets:
    eth0:
      dhcp4: false
      addresses:
        - 10.0.0.200/24
      gateway4: 10.0.0.10
      nameservers:
        addresses:
          - 10.0.0.10
          - 8.8.8.8
        search:
          - internal.lan

users:
  - name: ubuntu
    sudo: ["ALL=(ALL) NOPASSWD:ALL"]
    groups: users, admin
    shell: /bin/bash
    ssh_authorized_keys:
      - ${ssh_keys}

ssh_pwauth: false

package_update: true
package_upgrade: true
packages:
  - qemu-guest-agent
  - apt-transport-https
  - ca-certificates
  - curl
  - gnupg-agent
  - software-properties-common
  - zsh
  - nfs-common
  - gnupg
  - lsb-release
  - nftables
  - linux-firmware
  - alsa-utils
  - ubuntu-drivers-common
  - dnsmasq
  - ethtool
  - jq

growpart:
  mode: auto
  devices: ["/"]

write_files:
  - path: /etc/systemd/system/initial-setup.service
    permissions: '0644'
    content: |
      [Unit]
      Description=Post-bootstrap setup for Kubernetes node
      After=network-online.target
      Wants=network-online.target

      [Service]
      Type=oneshot
      EnvironmentFile=-/etc/environment
      ExecStart=/usr/local/bin/initial-setup.sh
      RemainAfterExit=true

      [Install]
      WantedBy=multi-user.target

  - path: /usr/local/bin/initial-setup.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      set -e  # Exit on any error
      exec > >(tee -a /var/log/initial-setup.log) 2>&1  # Log all output
      
      echo "$(date): Starting initial setup..."
      
      # Function to run commands with error handling
      run_with_error_handling() {
        local cmd="$1"
        local description="$2"
        echo "$(date): $description..."
        if eval "$cmd"; then
          echo "$(date): ✅ $description completed successfully"
        else
          echo "$(date): ⚠️  $description failed, but continuing..."
        fi
      }
      
      # export HTTP_PROXY=http://10.0.0.10:3128
      # export HTTPS_PROXY=http://10.0.0.10:3128
      # export NO_PROXY=127.0.0.1,localhost,10.0.0.0/24,10.96.0.0/12

      CPU_FLAGS="nohz_full=0-59 isolcpus=0-59 rcu_nocbs=0-59"
      sed -i "s|^GRUB_CMDLINE_LINUX=.*|GRUB_CMDLINE_LINUX=\"$CPU_FLAGS\"|" /etc/default/grub
      update-grub
      grub-mkconfig -o /boot/grub/grub.cfg || true

      # update-ca-certificates
      systemctl daemon-reload
      echo "$(date): Running performance tuning..."
      /usr/local/bin/performance-tuning.sh
      sysctl --system
      update-grub
      echo "$(date): Disabling swap..."
      /usr/local/bin/disable-swap.sh
      echo "$(date): Setting up Docker..."
      /usr/local/bin/setup-docker.sh
      echo "$(date): Setting up Fluent Bit..."
      /usr/local/bin/setup-fluentbit.sh
      systemctl enable setup-hosts-dns.service
      systemctl start setup-hosts-dns.service
      echo "$(date): Setting up utility services..."
      if /usr/local/bin/setup-utility.sh; then
        echo "$(date): ✅ Utility services setup completed successfully"
      else
        echo "$(date): ⚠️  Utility services setup failed, but system is functional"
      fi

      systemctl enable qemu-guest-agent
      systemctl start qemu-guest-agent
      systemctl daemon-reload
      
      echo "$(date): Initial setup completed successfully!"

  - path: /usr/local/bin/setup-utility.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      set -e  # Exit on any error
      exec > >(tee -a /var/log/setup-utility.log) 2>&1  # Log all output
      
      echo "$(date): 🔧 Setting up utility VM..."

      # Setup dedicated Harbor disk
      echo "💾 Setting up dedicated Harbor disk..."
      
      # Format the second disk (100GB) for Harbor storage
      HARBOR_DISK="/dev/sdb"
      HARBOR_MOUNT="/data/harbor"
      
      # Check if disk exists
      if [ -b "$HARBOR_DISK" ]; then
        echo "Found Harbor disk: $HARBOR_DISK"
        
        # Create filesystem if not already formatted
        if ! blkid "$HARBOR_DISK"; then
          echo "Formatting Harbor disk with ext4..."
          mkfs.ext4 -F "$HARBOR_DISK"
        else
          echo "Harbor disk already formatted"
        fi
        
        # Create mount point
        mkdir -p "$HARBOR_MOUNT"
        
        # Get UUID for persistent mounting
        DISK_UUID=$(blkid -s UUID -o value "$HARBOR_DISK")
        
        # Add to fstab if not already present
        if ! grep -q "$DISK_UUID" /etc/fstab; then
          echo "UUID=$DISK_UUID $HARBOR_MOUNT ext4 defaults,noatime 0 2" >> /etc/fstab
          echo "Added Harbor disk to fstab"
        fi
        
        # Mount the disk
        mount -a
        
        # Verify mount
        if mountpoint -q "$HARBOR_MOUNT"; then
          echo "✅ Harbor disk mounted successfully at $HARBOR_MOUNT"
          df -h "$HARBOR_MOUNT"
        else
          echo "❌ Failed to mount Harbor disk"
          exit 1
        fi
        
        # Set proper ownership and permissions
        chown -R root:root "$HARBOR_MOUNT"
        chmod 755 "$HARBOR_MOUNT"
        
      else
        echo "⚠️  Harbor disk $HARBOR_DISK not found, using default storage"
        mkdir -p /data/harbor
      fi

      # Harbor installation and configuration
      
      echo "📦 Installing Harbor ${HARBOR_VERSION}..."
      
      # Create harbor directories
      mkdir -p /opt/harbor
      mkdir -p /data/harbor
      
      # Download Harbor
      cd /tmp
      wget -q https://github.com/goharbor/harbor/releases/download/${HARBOR_VERSION}/harbor-offline-installer-${HARBOR_VERSION}.tgz
      tar xvf harbor-offline-installer-${HARBOR_VERSION}.tgz
      cp -r harbor/* /opt/harbor/
      
      # Create Harbor configuration
      cd /opt/harbor
      cp harbor.yml.tmpl harbor.yml
      
      # Use the pre-created template and configure Harbor
      HARBOR_HOSTNAME="$(hostname).internal.lan"
      cp /opt/harbor/harbor.yml.template /opt/harbor/harbor.yml
      
      # Replace the hostname placeholder
      sed -i "s/HOSTNAME_PLACEHOLDER/$${HARBOR_HOSTNAME}/g" /opt/harbor/harbor.yml
      
      # Create required directories for Harbor logging on dedicated disk
      mkdir -p /data/harbor/logs/jobs
      mkdir -p /data/harbor/logs
      
      # Install Harbor
      echo "🚀 Installing Harbor..."
      /opt/harbor/install.sh --with-trivy
      
      # Wait for Harbor to start
      echo "⏳ Waiting for Harbor to start..."
      sleep 30
      
      # Check and fix jobservice if needed
      echo "🔧 Checking Harbor jobservice health..."
      cd /opt/harbor
      
      # Wait for all Harbor services to be ready
      echo "⏳ Waiting for Harbor services to be ready..."
      while true; do
        if docker-compose ps | grep -q "Up" && docker-compose ps | grep -q "jobservice"; then
          echo "✅ Harbor services are running"
          break
        fi
        echo "Waiting for Harbor services..."
        sleep 10
      done
      
      # Check jobservice specifically
      JOBSERVICE_STATUS=$(docker ps --filter "name=jobservice" --format "table {{.Status}}" | grep -c "Up" || echo "0")
      if [ "$JOBSERVICE_STATUS" -eq 0 ]; then
        echo "⚠️  Jobservice not running properly, restarting Harbor..."
        docker-compose down
        sleep 10
        docker-compose up -d
        sleep 30
        
        # Verify jobservice again
        RETRY_STATUS=$(docker ps --filter "name=jobservice" --format "table {{.Status}}" | grep -c "Up" || echo "0")
        if [ "$RETRY_STATUS" -eq 0 ]; then
          echo "❌ Jobservice still failing, checking logs..."
          docker-compose logs | grep jobservice
          echo "Attempting jobservice container restart..."
          JOBSERVICE_CONTAINER=$(docker ps -a --filter "name=jobservice" --format "{{.Names}}" | head -1)
          if [ ! -z "$JOBSERVICE_CONTAINER" ]; then
            docker restart "$JOBSERVICE_CONTAINER"
            sleep 15
          else
            echo "❌ Could not find jobservice container to restart"
          fi
        else
          echo "✅ Jobservice recovered after restart"
        fi
      else
        echo "✅ Jobservice is running properly"
      fi
      
      # Configure Harbor as pull-through cache
      echo "🔧 Configuring Harbor as pull-through cache..."
      
      # Create proxy cache projects for common registries
      HARBOR_HOST="localhost"
      HARBOR_URL="http://$HARBOR_HOST"
      export HARBOR_USER=${HARBOR_USER}
      export HARBOR_PASS=${HARBOR_PASS}

      
      # Wait for Harbor API to be available with better timing
      echo "⏳ Waiting for Harbor API to be fully ready..."
      sleep 60  # Give Harbor more time to fully initialize
      
      until curl -f -s "$HARBOR_URL/api/v2.0/systeminfo" > /dev/null; do
        echo "Waiting for Harbor API to be available..."
        sleep 15
      done
      
      # Wait additional time for Harbor to be fully operational
      echo "✅ Harbor API available, waiting for full initialization..."
      sleep 30
      
      # Function to create proxy cache project
      create_proxy_project() {
        local project_name=$1
        local registry_url=$2
        local registry_name=$3
        
        echo "Creating proxy cache project: $project_name"
        
        # Create registry endpoint first
        echo "Creating registry endpoint: $registry_name"
        REGISTRY_RESPONSE=$(curl -s -w "\n%%{http_code}" -X POST "$HARBOR_URL/api/v2.0/registries" \
          -H "Content-Type: application/json" \
          -u "${HARBOR_USER}:${HARBOR_PASS}" \
          -d "{
            \"name\": \"$registry_name\",
            \"type\": \"docker-registry\",
            \"url\": \"$registry_url\",
            \"insecure\": false,
            \"credential\": {
              \"type\": \"basic\",
              \"access_key\": \"\",
              \"access_secret\": \"\"
            }
          }")
        
        HTTP_CODE=$(echo "$REGISTRY_RESPONSE" | tail -n1)
        RESPONSE_BODY=$(echo "$REGISTRY_RESPONSE" | head -n -1)
        
        if [ "$HTTP_CODE" != "201" ] && [ "$HTTP_CODE" != "409" ]; then
          echo "❌ Failed to create registry $registry_name (HTTP $HTTP_CODE): $RESPONSE_BODY"
          return 1
        fi
        
        # Wait a moment for registry to be created
        sleep 5
        
        # Get registry ID
        echo "Getting registry ID for: $registry_name"
        REGISTRY_ID=""
        while true; do
          REGISTRY_ID=$(curl -s "$HARBOR_URL/api/v2.0/registries" \
            -u "${HARBOR_USER}:${HARBOR_PASS}" | \
            jq -r ".[] | select(.name==\"$registry_name\") | .id" 2>/dev/null)
          
          if [ ! -z "$REGISTRY_ID" ] && [ "$REGISTRY_ID" != "null" ]; then
            echo "✅ Found registry ID: $REGISTRY_ID"
            break
          fi
          echo "Waiting for registry to be available..."
          sleep 3
        done
        
        if [ -z "$REGISTRY_ID" ] || [ "$REGISTRY_ID" == "null" ]; then
          echo "❌ Failed to get registry ID for $registry_name"
          return 1
        fi
        
        # Create proxy cache project
        echo "Creating proxy cache project: $project_name with registry ID: $REGISTRY_ID"
        PROJECT_RESPONSE=$(curl -s -w "\n%%{http_code}" -X POST "$HARBOR_URL/api/v2.0/projects" \
          -H "Content-Type: application/json" \
          -u "${HARBOR_USER}:${HARBOR_PASS}" \
          -d "{
            \"project_name\": \"$project_name\",
            \"registry_id\": $REGISTRY_ID,
            \"metadata\": {
              \"public\": \"true\"
            }
          }")
        
        PROJECT_HTTP_CODE=$(echo "$PROJECT_RESPONSE" | tail -n1)
        PROJECT_RESPONSE_BODY=$(echo "$PROJECT_RESPONSE" | head -n -1)
        
        if [ "$PROJECT_HTTP_CODE" != "201" ] && [ "$PROJECT_HTTP_CODE" != "409" ]; then
          echo "❌ Failed to create project $project_name (HTTP $PROJECT_HTTP_CODE): $PROJECT_RESPONSE_BODY"
          return 1
        fi
        
        echo "✅ Successfully created proxy cache project: $project_name"
        sleep 2  # Brief pause between projects
      }
      
      # Create proxy cache projects for popular registries
      create_proxy_project "dockerhub" "https://registry-1.docker.io" "docker-hub"
      create_proxy_project "gcr" "https://gcr.io" "google-gcr"
      create_proxy_project "quay" "https://quay.io" "quay-io"
      create_proxy_project "ghcr" "https://ghcr.io" "github-container-registry"
      create_proxy_project "k8s-gcr" "https://k8s.gcr.io" "k8s-gcr-io"
      create_proxy_project "registry-k8s" "https://registry.k8s.io" "registry-k8s-io"
      
      echo "✅ Harbor installation completed!"
      echo "🌐 Harbor is available at: http://$HARBOR_HOST"
      echo "🌐 External Harbor URL: https://harbor.${RANCHER_DOMAIN}"
      echo "🔐 Harbor authentication: GitHub OAuth only (database auth disabled)"
      echo "📦 Proxy cache: Anonymous read access enabled for pull-through functionality"
      echo "⚠️  IMPORTANT: Harbor GUI requires GitHub OAuth authentication"
      echo "   • Only users from GitHub organization '${GITHUB_ORG}' can access the GUI"
      echo "   • Members of '${GITHUB_ORG}:harbor_admins' have administrative privileges"
      echo "   • Proxy cache projects support anonymous pulls (no authentication required)"
      echo ""
      echo "🔧 Use 'harbor-manage' command to manage Harbor service"
      echo ""
      echo "📋 Proxy cache projects created:"
      echo "  - dockerhub (Docker Hub): $HARBOR_HOST/dockerhub/"
      echo "  - gcr (Google Container Registry): $HARBOR_HOST/gcr/"
      echo "  - quay (Quay.io): $HARBOR_HOST/quay/"
      echo "  - ghcr (GitHub Container Registry): $HARBOR_HOST/ghcr/"
      echo "  - k8s-gcr (Kubernetes GCR): $HARBOR_HOST/k8s-gcr/"
      echo "  - registry-k8s (Kubernetes Registry): $HARBOR_HOST/registry-k8s/"
      echo ""
      echo "🚀 Configure your Kubernetes cluster to use Harbor as pull-through cache:"
      echo "  Example: docker pull $HARBOR_HOST/dockerhub/nginx:latest"
      echo ""
      
      # Make the debug-oidc script executable (script created via cloud-init write_files)
      chmod +x /usr/local/bin/debug-oidc
      
      # Deploy Dex OIDC provider if GitHub OAuth credentials are provided
      if [ -n "${HARBOR_OIDC_CLIENT}" ] && [ -n "${HARBOR_OIDC_SECRET}" ]; then
        echo "🔧 Deploying Dex OIDC provider for GitHub OAuth integration..."
        if /usr/local/bin/setup-dex.sh; then
          echo "✅ Dex deployment successful"
          
          # Configure Harbor to use Dex OIDC
          echo "🔧 Configuring Harbor to use Dex OIDC..."
          sleep 10  # Wait for Dex to be fully ready
          
          # Get the Dex client secret that was generated
          DEX_CLIENT_SECRET=$(grep "secret:" /opt/dex/config.yaml | sed "s/.*secret: '//" | sed "s/'.*//")
          
          # Configure Harbor OIDC settings and disable database authentication
          OIDC_CONFIG_RESPONSE=$(curl -s -w "\n%%{http_code}" -X PUT "$HARBOR_URL/api/v2.0/configurations" \
            -H "Content-Type: application/json" \
            -u "${HARBOR_USER}:${HARBOR_PASS}" \
            -d "{
              \"auth_mode\": \"oidc_auth\",
              \"oidc_name\": \"GitHub via Dex\",
              \"oidc_endpoint\": \"https://harbor-dex.${RANCHER_DOMAIN}\",
              \"oidc_client_id\": \"harbor\",
              \"oidc_client_secret\": \"$DEX_CLIENT_SECRET\",
              \"oidc_scope\": \"openid email profile groups\",
              \"oidc_verify_cert\": false,
              \"oidc_auto_onboard\": true,
              \"oidc_user_claim\": \"email\",
              \"oidc_groups_claim\": \"groups\",
              \"oidc_admin_group\": \"${GITHUB_ORG}:harbor_admins\",
              \"project_creation_restriction\": \"adminonly\",
              \"self_registration\": false,
              \"read_only\": false,
              \"auth_proxy_settings\": {
                \"endpoint\": \"\",
                \"token_review_endpoint\": \"\",
                \"skip_search\": false
              }
            }")
          
          OIDC_HTTP_CODE=$(echo "$OIDC_CONFIG_RESPONSE" | tail -n1)
          OIDC_RESPONSE_BODY=$(echo "$OIDC_CONFIG_RESPONSE" | head -n -1)
          
          echo "🔧 Harbor OIDC Configuration Details:"
          echo "  Request URL: $HARBOR_URL/api/v2.0/configurations"
          echo "  HTTP Response Code: $OIDC_HTTP_CODE"
          echo "  Dex Client Secret: $DEX_CLIENT_SECRET"
          echo "  OIDC Endpoint: https://harbor-dex.${RANCHER_DOMAIN}"
          
          if [ "$OIDC_HTTP_CODE" = "200" ]; then
            echo "✅ Harbor configured to use Dex OIDC for GitHub authentication"
            echo "🔗 GitHub OIDC Login URL: https://harbor.${RANCHER_DOMAIN}/c/oidc/login"
            echo "👤 Users can now login with their GitHub accounts"
            
            # Verify the configuration was applied
            echo "🔧 Verifying OIDC configuration..."
            sleep 5
            VERIFY_CONFIG=$(curl -s "$HARBOR_URL/api/v2.0/configurations" -u "${HARBOR_USER}:${HARBOR_PASS}")
            echo "Current auth_mode: $(echo "$VERIFY_CONFIG" | jq -r '.auth_mode.value // .auth_mode' 2>/dev/null)"
            echo "Current oidc_endpoint: $(echo "$VERIFY_CONFIG" | jq -r '.oidc_endpoint.value // .oidc_endpoint' 2>/dev/null)"
            echo "Current oidc_client_id: $(echo "$VERIFY_CONFIG" | jq -r '.oidc_client_id.value // .oidc_client_id' 2>/dev/null)"
            
            # Configure anonymous read access for proxy cache functionality
            echo "🔧 Ensuring anonymous read access for proxy cache..."
            ANONYMOUS_CONFIG_RESPONSE=$(curl -s -w "\n%%{http_code}" -X PUT "$HARBOR_URL/api/v2.0/configurations" \
              -H "Content-Type: application/json" \
              -u "${HARBOR_USER}:${HARBOR_PASS}" \
              -d "{
                \"auth_mode\": \"oidc_auth\",
                \"project_creation_restriction\": \"adminonly\",
                \"self_registration\": false
              }")
            
            ANON_HTTP_CODE=$(echo "$ANONYMOUS_CONFIG_RESPONSE" | tail -n1)
            if [ "$ANON_HTTP_CODE" = "200" ]; then
              echo "✅ Anonymous read access configured for proxy cache projects"
            else
              echo "⚠️  Anonymous access configuration warning (HTTP $ANON_HTTP_CODE)"
            fi
            
            echo "🔒 Finalizing security: Ensuring database authentication is fully disabled..."
            # Final verification that only OIDC auth is enabled
            FINAL_AUTH_CONFIG=$(curl -s -w "\n%%{http_code}" -X PUT "$HARBOR_URL/api/v2.0/configurations" \
              -H "Content-Type: application/json" \
              -u "${HARBOR_USER}:${HARBOR_PASS}" \
              -d "{
                \"auth_mode\": \"oidc_auth\",
                \"self_registration\": false,
                \"project_creation_restriction\": \"adminonly\"
              }")
            
            FINAL_HTTP_CODE=$(echo "$FINAL_AUTH_CONFIG" | tail -n1)
            if [ "$FINAL_HTTP_CODE" = "200" ]; then
              echo "🔒 Database authentication disabled - Harbor GUI now requires GitHub OAuth"
              echo "📦 Proxy cache projects retain anonymous read access for container pulls"
            else
              echo "⚠️  Final auth configuration warning (HTTP $FINAL_HTTP_CODE)"
            fi
          else
            echo "⚠️  Harbor OIDC configuration failed (HTTP $OIDC_HTTP_CODE)"
            echo "Response: $OIDC_RESPONSE_BODY"
            echo "Harbor will continue using database authentication"
            
            # Try to get more details about the error
            echo "🔧 Checking Harbor logs for errors..."
            cd /opt/harbor
            docker-compose logs --tail=10 core | grep -i error || echo "No recent errors found in Harbor core logs"
          fi
          
          echo "✅ GitHub OAuth authentication via Dex configured successfully"
          echo ""
          echo "🔒 SECURITY SUMMARY:"
          echo "   ✅ Database authentication: DISABLED (secure GUI access)"
          echo "   ✅ GitHub OAuth: ENABLED (via Dex OIDC provider)"
          echo "   ✅ Anonymous read access: ENABLED (for proxy cache pulls)"
          echo "   ✅ Project creation: ADMIN ONLY"
          echo "   ✅ Self registration: DISABLED"
          echo "   🌐 GUI Access: https://harbor.${RANCHER_DOMAIN}/c/oidc/login"
          echo "   📦 Proxy Cache: docker pull harbor.${RANCHER_DOMAIN}/dockerhub/image:tag"
          echo ""
          
          # Make the test-oidc-flow script executable (script created via cloud-init write_files)
          chmod +x /usr/local/bin/test-oidc-flow
          
          echo "🔧 Created debugging tools:"
          echo "  - debug-oidc: Show current OIDC configuration and status"
          echo "  - test-oidc-flow: Test the complete OIDC authentication flow"
        else
          echo "⚠️  Dex deployment failed, Harbor will use database authentication"
        fi
      else
        echo "⚠️  No GitHub OAuth credentials provided"
        echo "🔒 SECURITY WARNING: Harbor GUI will be inaccessible without GitHub OAuth"
        echo "📦 Proxy cache projects will still work with anonymous read access"
        echo "ℹ️  To enable GitHub OAuth GUI access:"
        echo "   • Set HARBOR_OIDC_CLIENT to your GitHub OAuth App Client ID"
        echo "   • Set HARBOR_OIDC_SECRET to your GitHub OAuth App Client Secret"
        echo "   • Set GITHUB_ORG to your GitHub organization name"
        echo "🔐 Current configuration: OIDC-only authentication (no database auth)"
      fi
      
      # Make the harbor-manage script executable (script created via cloud-init write_files)
      chmod +x /usr/local/bin/harbor-manage
      
      echo "✅ Harbor setup completed successfully"
      
      # Setup Tailscale
      echo "🔧 Setting up Tailscale..."
      if /usr/local/bin/setup-tailscale.sh; then
        echo "✅ Tailscale setup completed successfully"
      else
        echo "⚠️  Tailscale setup failed, but system is functional"
      fi

  - path: /usr/local/bin/setup-dex.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      set -e
      exec > >(tee -a /var/log/setup-dex.log) 2>&1  # Log all output
      
      echo "$(date): 🔧 Installing and configuring Dex OIDC provider..."
      
      # Validate required environment variables
      if [ -z "${HARBOR_OIDC_CLIENT}" ] || [ -z "${HARBOR_OIDC_SECRET}" ] || [ -z "${RANCHER_DOMAIN}" ] || [ -z "${GITHUB_ORG}" ]; then
        echo "❌ Missing required environment variables for Dex:"
        echo "  HARBOR_OIDC_CLIENT: ${HARBOR_OIDC_CLIENT}"
        echo "  HARBOR_OIDC_SECRET: ${HARBOR_OIDC_SECRET}"
        echo "  RANCHER_DOMAIN: ${RANCHER_DOMAIN}"
        echo "  GITHUB_ORG: ${GITHUB_ORG}"
        echo "⚠️  Skipping Dex installation due to missing configuration"
        exit 1
      fi
      
      echo "$(date): ✅ Environment variables validated"
      
      # Create directories
      mkdir -p /opt/dex /data/dex
      
      # Deploy Dex as a Docker container
      cd /tmp
      
      echo "$(date): � Deploying Dex OIDC provider as Docker container..."
      
      # Create directories for Dex data and config
      mkdir -p /opt/dex /data/dex
      
      # Generate client secret for Harbor
      DEX_CLIENT_SECRET=$(openssl rand -hex 16)
      echo "$(date): Generated Dex client secret for Harbor"
      
      # Create Dex configuration from template
      cp /opt/dex/config.yaml.template /opt/dex/config.yaml
      
      # Replace placeholder with actual secret
      sed -i "s/DEX_CLIENT_SECRET_PLACEHOLDER/$DEX_CLIENT_SECRET/" /opt/dex/config.yaml
      
      # Debug: Show final configuration (redacted)
      echo "$(date): Dex configuration summary:"
      echo "  - Issuer: $(grep '^issuer:' /opt/dex/config.yaml || echo 'NOT FOUND')"
      echo "  - GitHub Client ID: $(grep 'clientID:' /opt/dex/config.yaml | sed 's/.*clientID: *//' || echo 'NOT FOUND')"
      echo "  - GitHub Org: $(grep 'name:.*${GITHUB_ORG}' /opt/dex/config.yaml || echo 'NOT FOUND')"
      echo "  - Harbor redirect URI: $(grep 'harbor.*callback' /opt/dex/config.yaml || echo 'NOT FOUND')"
      echo "  - Storage file: $(grep 'file:' /opt/dex/config.yaml || echo 'NOT FOUND')"
      
      # Create dex user and ensure proper directory permissions
      useradd --system --shell /bin/false --home-dir /data/dex dex 2>/dev/null || true
      
      # Ensure directories exist with proper permissions for container access
      mkdir -p /data/dex /opt/dex
      chmod -R 755 /data/dex /opt/dex
      
      # Create the database file directory explicitly
      mkdir -p /data/dex
      touch /data/dex/dex.db 2>/dev/null || true
      chmod 666 /data/dex/dex.db 2>/dev/null || true
      
      # Pull Dex Docker image
      DEX_IMAGE="dexidp/dex:v2.30.0"
      echo "$(date): Pulling Dex Docker image: $DEX_IMAGE"
      
      if ! docker pull "$DEX_IMAGE"; then
        echo "⚠️  Failed to pull $DEX_IMAGE, trying latest tag..."
        DEX_IMAGE="dexidp/dex:latest"
        if ! docker pull "$DEX_IMAGE"; then
          echo "❌ Failed to pull Dex Docker image"
          exit 1
        fi
      fi
      
      echo "✅ Dex Docker image pulled successfully"
      
      # Create Docker Compose file from template
      cp /opt/dex/docker-compose.yml.template /opt/dex/docker-compose.yml
      
      # Start Dex container
      echo "$(date): Starting Dex container..."
      cd /opt/dex
      
      # Debug: Check directory permissions before starting
      echo "$(date): Checking /data/dex permissions:"
      ls -la /data/dex/
      
      if docker-compose up -d; then
        echo "✅ Dex container started successfully"
      else
        echo "❌ Failed to start Dex container"
        echo "Docker logs:"
        docker-compose logs
        exit 1
      fi
      
      # Wait for Dex to be ready - infinite loop until success
      echo "$(date): ⏳ Waiting for Dex to be ready..."
      attempt=1
      while true; do
        # Try the OIDC discovery endpoint (this is the key endpoint)
        if curl -f -s http://localhost:5556/.well-known/openid-configuration >/dev/null 2>&1; then
          echo "✅ Dex OIDC provider is ready (attempt $attempt) - OIDC discovery endpoint responding"
          break
        fi
        
        # Check Docker health status
        if docker-compose ps | grep -q "healthy"; then
          echo "✅ Dex OIDC provider is ready (attempt $attempt) - Docker health check passed"
          break
        fi
        
        # Try the health endpoint
        if curl -f -s http://localhost:5556/healthz >/dev/null 2>&1; then
          echo "✅ Dex OIDC provider is ready (attempt $attempt) - /healthz endpoint responding"
          break
        fi
        
        # Try some alternative endpoint patterns
        if curl -f -s http://localhost:5556/auth >/dev/null 2>&1; then
          echo "✅ Dex OIDC provider is ready (attempt $attempt) - /auth endpoint responding"
          break
        fi
        
        # Fallback to basic connectivity test
        if curl -f -s http://localhost:5556/ >/dev/null 2>&1; then
          echo "✅ Dex OIDC provider is ready (attempt $attempt) - main endpoint responding"
          break
        fi
        
        echo "Waiting for Dex to be ready... (attempt $attempt)"
        if [ $((attempt % 10)) -eq 0 ]; then
          echo "🔧 Current container status (attempt $attempt):"
          docker-compose ps || true
          echo "🔧 Latest container logs:"
          docker-compose logs --tail=5 || true
          echo "🔧 Testing primary endpoints:"
          echo "  - /.well-known/openid-configuration: $(curl -s -o /dev/null -w '%%{http_code}' http://localhost:5556/.well-known/openid-configuration 2>/dev/null || echo 'failed')"
          echo "  - /healthz: $(curl -s -o /dev/null -w '%%{http_code}' http://localhost:5556/healthz 2>/dev/null || echo 'failed')"
          echo "🔧 Checking if container is still running..."
          if ! docker-compose ps | grep -q "Up"; then
            echo "❌ Container is no longer running! Breaking out of wait loop."
            break
          fi
        fi
        attempt=$((attempt + 1))
        sleep 2
      done
      
      # Final status check
      CONTAINER_RUNNING=$(docker-compose ps | grep -c "Up" || echo "0")
      
      if [ "$CONTAINER_RUNNING" -gt 0 ]; then
        echo "✅ Dex OIDC provider is fully operational"
        echo "🌐 Dex issuer: https://harbor-dex.${RANCHER_DOMAIN}"
        echo "🔧 Health endpoint: http://localhost:5556/healthz"
        echo "🔧 OIDC discovery: http://localhost:5556/.well-known/openid-configuration"
        echo "🔧 Use 'cd /opt/dex && docker-compose logs -f' to view logs"
        echo "🔧 Use 'cd /opt/dex && docker-compose restart' to restart Dex"
        exit 0
      else
        echo "❌ Dex container failed to start properly"
        echo "Container status:"
        docker-compose ps
        echo "Container logs:"
        docker-compose logs
        exit 1
      fi

  - path: /usr/local/bin/setup-tailscale.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      set -e
      exec > >(tee -a /var/log/setup-tailscale.log) 2>&1  # Log all output
      
      echo "$(date): 🔧 Installing and configuring Tailscale..."
      
      # Install Tailscale
      echo "$(date): 📦 Installing Tailscale..."
      curl -fsSL https://tailscale.com/install.sh | sh
      
      # Check if Tailscale auth key is provided
      if [ -n "${TAILSCALE_AUTH_KEY}" ]; then
        echo "$(date): 🔑 Authenticating Tailscale with provided auth key..."
        sudo tailscale up --authkey="${TAILSCALE_AUTH_KEY}" --advertise-routes=10.0.0.0/24 --accept-routes
        
        # Enable IP forwarding for subnet routing
        echo 'net.ipv4.ip_forward = 1' | sudo tee -a /etc/sysctl.conf
        echo 'net.ipv6.conf.all.forwarding = 1' | sudo tee -a /etc/sysctl.conf
        sudo sysctl -p
        
        echo "✅ Tailscale connected and configured as subnet router"
        echo "🌐 This node advertises routes for: 10.0.0.0/24"
        echo "🌐 This node accepts routes from other Tailscale nodes"
        
        # Get Tailscale status
        TAILSCALE_IP=$(tailscale ip -4 2>/dev/null || echo "Unknown")
        echo "🔧 Tailscale IP: $TAILSCALE_IP"
        
        # Configure Tailscale Funnel if enabled
        if [ "${TAILSCALE_FUNNEL_ENABLED}" = "true" ]; then
          echo "$(date): 🌐 Configuring Tailscale Funnel for multiple services..."
          
          # Get Tailscale hostname
          TAILSCALE_HOSTNAME=$(tailscale status --json | jq -r '.Self.DNSName' | sed 's/\.$//')
          
          if [ -n "$TAILSCALE_HOSTNAME" ] && [ "$TAILSCALE_HOSTNAME" != "null" ]; then
            echo "$(date): 🔧 Setting up Funnel services at https://$TAILSCALE_HOSTNAME"
            
            # Configure Tailscale serve for multiple services using new CLI syntax
            # Harbor on default HTTPS port (443)
            sudo tailscale serve --bg --https=443 http://localhost:80
            
            # Rancher GUI on port 8443
            sudo tailscale serve --bg --https=8443 https://k8s.${RANCHER_DOMAIN}:443
            
            # APISIX Dashboard on port 9000
            sudo tailscale serve --bg --https=9000 http://localhost:9000
            
            # Enable Funnel for all configured ports
            sudo tailscale funnel --bg --https=443
            sudo tailscale funnel --bg --https=8443
            sudo tailscale funnel --bg --https=9000
            
            echo "✅ Tailscale Funnel configured for multiple services"
            echo "🌐 Services accessible via:"
            echo "   • Harbor Container Registry:    https://$TAILSCALE_HOSTNAME"
            echo "   • Rancher Kubernetes GUI:       https://$TAILSCALE_HOSTNAME:8443"
            echo "   • APISIX API Gateway Dashboard: https://$TAILSCALE_HOSTNAME:9000"
            echo "🔐 All services provide secure access from anywhere on the internet"
            echo "⚠️  Note: Funnel traffic goes through Tailscale's infrastructure"
            
          else
            echo "⚠️  Could not determine Tailscale hostname for Funnel setup"
          fi
        else
          echo "ℹ️  Tailscale Funnel not enabled (set TAILSCALE_FUNNEL_ENABLED=true to enable)"
        fi
        
      else
        echo "⚠️  No Tailscale auth key provided"
        echo "ℹ️  Tailscale installed but not connected"
        echo "ℹ️  To connect manually:"
        echo "   1. Run: sudo tailscale up"
        echo "   2. Follow the authentication URL"
        echo "   3. Optionally enable subnet routing: sudo tailscale up --advertise-routes=10.0.0.0/24"
      fi
      
      # Create tailscale management script
      chmod +x /usr/local/bin/tailscale-manage
      
      echo "✅ Tailscale setup completed"

  - path: /usr/local/bin/tailscale-manage
    permissions: '0755'
    content: |
      #!/bin/bash
      case "$1" in
        status)
          echo "=== Tailscale Status ==="
          tailscale status
          echo ""
          echo "=== Tailscale IP ==="
          tailscale ip -4 2>/dev/null || echo "Not connected"
          echo ""
          echo "=== Route Advertisement ==="
          tailscale status --json | jq -r '.Self.PrimaryRoutes[]? // "No routes advertised"'
          echo ""
          echo "=== Available Services ==="
          if tailscale funnel status >/dev/null 2>&1; then
            TAILSCALE_HOSTNAME=$(tailscale status --json | jq -r '.Self.DNSName' | sed 's/\.$//')
            echo "  • Harbor Registry:       https://$TAILSCALE_HOSTNAME"
            echo "  • Rancher GUI:           https://$TAILSCALE_HOSTNAME:8843"
            echo "  • APISIX Dashboard:      https://$TAILSCALE_HOSTNAME:9000"
          else
            echo "  Funnel not enabled - services accessible via Tailscale network only"
          fi
          ;;
        services)
          echo "=== Available Services ==="
          echo "• Harbor Container Registry"
          echo "  Internal: http://utility-node.internal.lan"
          echo "  External: https://harbor.${RANCHER_DOMAIN}"
          if tailscale status >/dev/null 2>&1; then
            TAILSCALE_HOSTNAME=$(tailscale status --json | jq -r '.Self.DNSName' | sed 's/\.$//')
            echo "  Tailscale: https://$TAILSCALE_HOSTNAME"
          fi
          echo ""
          echo "• Rancher Kubernetes Management"
          echo "  Internal: https://k8s.${RANCHER_DOMAIN}:443"
          if tailscale status >/dev/null 2>&1; then
            TAILSCALE_HOSTNAME=$(tailscale status --json | jq -r '.Self.DNSName' | sed 's/\.$//')
            echo "  Tailscale: https://$TAILSCALE_HOSTNAME:8443"
          fi
          echo ""
          echo "• APISIX API Gateway Dashboard"
          echo "  Internal: http://localhost:9000"
          if tailscale status >/dev/null 2>&1; then
            TAILSCALE_HOSTNAME=$(tailscale status --json | jq -r '.Self.DNSName' | sed 's/\.$//')
            echo "  Tailscale: https://$TAILSCALE_HOSTNAME:9000"
          fi
          ;;
        up)
          if [ -n "$2" ]; then
            echo "Connecting Tailscale with auth key..."
            sudo tailscale up --authkey="$2" --advertise-routes=10.0.0.0/24 --accept-routes
          else
            echo "Connecting Tailscale (will need browser authentication)..."
            sudo tailscale up --advertise-routes=10.0.0.0/24 --accept-routes
          fi
          ;;
        down)
          echo "Disconnecting Tailscale..."
          sudo tailscale down
          ;;
        routes)
          echo "=== Advertised Routes ==="
          tailscale status --json | jq -r '.Self.PrimaryRoutes[]? // "No routes advertised"'
          echo ""
          echo "=== Accepted Routes ==="
          tailscale status --json | jq -r '.Peer[].PrimaryRoutes[]? // "No accepted routes"'
          ;;
        funnel-status)
          echo "=== Tailscale Funnel Status ==="
          tailscale funnel status 2>/dev/null || echo "Funnel not configured or not available"
          ;;
        funnel-enable)
          TAILSCALE_HOSTNAME=$(tailscale status --json | jq -r '.Self.DNSName' | sed 's/\.$//')
          if [ -n "$TAILSCALE_HOSTNAME" ] && [ "$TAILSCALE_HOSTNAME" != "null" ]; then
            echo "Enabling Tailscale Funnel for all services..."
            
            # Harbor on 443
            sudo tailscale serve --bg --https=443 http://localhost:80
            
            # Rancher on 8443
            sudo tailscale serve --bg --https=8443 https://k8s.${RANCHER_DOMAIN}:443
            
            # APISIX on 9000
            sudo tailscale serve --bg --https=9000 http://localhost:9000
            
            # Enable Funnel for all ports
            sudo tailscale funnel --bg --https=443
            sudo tailscale funnel --bg --https=8443
            sudo tailscale funnel --bg --https=9000
            
            echo "✅ Funnel enabled for all services"
            echo "🌐 Services accessible via:"
            echo "   • Harbor:    https://$TAILSCALE_HOSTNAME"
            echo "   • Rancher:   https://$TAILSCALE_HOSTNAME:8443"
            echo "   • APISIX:    https://$TAILSCALE_HOSTNAME:9000"
          else
            echo "❌ Could not determine Tailscale hostname"
          fi
          ;;
        funnel-disable)
          echo "Disabling Tailscale Funnel..."
          sudo tailscale funnel --bg --https=443 off
          sudo tailscale funnel --bg --https=8443 off
          sudo tailscale funnel --bg --https=9000 off
          sudo tailscale serve reset
          echo "✅ Funnel disabled for all services"
          ;;
        logs)
          echo "=== Tailscale Service Logs ==="
          journalctl -u tailscaled --no-pager -n 50
          ;;
        *)
          echo "Tailscale Management Script for Multi-Service Funnel"
          echo ""
          echo "Usage: $0 {status|services|up [auth-key]|down|routes|funnel-status|funnel-enable|funnel-disable|logs}"
          echo ""
          echo "Commands:"
          echo "  status         - Show Tailscale connection status and service URLs"
          echo "  services       - Show detailed service information with all access methods"
          echo "  up [auth-key]  - Connect to Tailscale (optionally with auth key)"
          echo "  down           - Disconnect from Tailscale"
          echo "  routes         - Show route advertisement status"
          echo "  funnel-status  - Show Tailscale Funnel status"
          echo "  funnel-enable  - Enable Tailscale Funnel for Harbor, Rancher, and APISIX"
          echo "  funnel-disable - Disable Tailscale Funnel"
          echo "  logs           - Show recent Tailscale service logs"
          echo ""
          echo "Services accessible via Tailscale Funnel:"
          echo "  • Harbor Container Registry (Port 443)"
          echo "  • Rancher Kubernetes Management GUI (Port 8443)"
          echo "  • APISIX API Gateway Dashboard (Port 9000)"
          echo ""
          echo "Examples:"
          echo "  $0 status                           # Check connection and service URLs"
          echo "  $0 services                         # Show all access methods for each service"
          echo "  $0 up tskey-auth-xxxxx             # Connect with auth key"  
          echo "  $0 funnel-enable                   # Enable public access to all services"
          exit 1
          ;;
      esac

  - path: /opt/harbor/harbor.yml.template
    permissions: '0644'
    content: |
      # Configuration file of Harbor
      
      # The IP address or hostname to access admin UI and registry service.
      hostname: HOSTNAME_PLACEHOLDER
      
      # http related config
      http:
        # port for http, default is 80. If https enabled, this port will redirect to https port
        port: 80
      
      # https related config - DISABLED for pull-through cache
      # https:
      #   port: 443
      #   certificate:
      #   private_key:
      
      # Uncomment external_url if you want to enable external proxy
      # And when it enabled the hostname will no longer used
      external_url: https://harbor.${RANCHER_DOMAIN}
      
      # The initial password of Harbor admin
      harbor_admin_password: Harbor12345
      
      # Authentication configuration
      # Using OIDC authentication for GitHub OAuth - database authentication will be disabled entirely
      auth_mode: oidc_auth
      
      # OIDC authentication settings - DISABLED due to GitHub OAuth compatibility issues
      # GitHub OAuth App integration requires manual configuration through Harbor UI
      # oidc_name: GitHub
      # oidc_endpoint: https://github.com/login/oauth/authorize
      # oidc_client_id: ${HARBOR_OIDC_CLIENT}
      # oidc_client_secret: ${HARBOR_OIDC_SECRET}
      # oidc_scope: read:user,read:org
      # oidc_verify_cert: true
      # oidc_auto_onboard: true
      # oidc_user_claim: login
      # oidc_groups_claim: organizations
      # oidc_admin_group: harbor_admins
      # oidc_group_filter: harbor_users,harbor_admins
      
      # Harbor DB configuration
      database:
        # The password for the root user of Harbor DB
        password: root123
        # The maximum number of connections in the idle connection pool (increased for jobservice)
        max_idle_conns: 100
        # The maximum number of open connections to the database (increased for jobservice)
        max_open_conns: 2000
        # Connection timeout for database operations
        conn_max_lifetime: 5m
        # Connection timeout for database connections
        conn_max_idle_time: 10m
      
      # The default data volume
      data_volume: /data/harbor

      # Harbor Storage settings by default is using /data dir on local filesystem
      # Uncomment storage_service setting If you want to using external storage
      # storage_service:
      #   # ca_bundle is the path to the custom root ca certificate, which will be injected into the truststore
      #   # of registry's and chart repository's containers.  This is usually needed when the user hosts a internal storage with self signed certificate.
      #   ca_bundle:
      
      #   # storage backend, default is filesystem, options include filesystem, azure, gcs, s3, swift and oss
      #   # for more info about this configuration please refer https://docs.docker.com/registry/configuration/
      #   filesystem:
      #     maxthreads: 100
      
      # Trivy configuration
      trivy:
        # ignoreUnfixed The flag to display only fixed vulnerabilities
        ignore_unfixed: false
        # skipUpdate The flag to enable or disable Trivy DB downloads from GitHub
        skip_update: false
        # The offline_scan option prevents Trivy from sending API requests to identify dependencies.
        offline_scan: false
        # insecure The flag to skip verifying registry certificate
        insecure: false
      
      jobservice:
        # Maximum number of job workers in job service (increased for better throughput)
        max_job_workers: 20
        # The timeout for webhook job (increased for large image operations)
        webhook_job_timeout: 300
        # Job processing timeout (5 minutes for large operations)
        job_timeout: 300
        # Pool size for job workers
        pool_size: 50
        # Memory settings for jobservice workers
        worker_memory_mb: 512
        # Maximum queue size for pending jobs
        max_queue_size: 10000
        # Job cleanup settings
        cleanup_timeout: 3600
        # Loggers for the job service
        job_loggers:
          - "STD_OUTPUT"
          - "FILE"
          - "DB"
        # Logger sweeper duration (clean logs every 24 hours)
        logger_sweeper_duration: 24
        # Retry settings for failed jobs
        job_retry_count: 3
        job_retry_delay: 10
        # Enable job metrics collection
        enable_metrics: true
      
      notification:
        # Maximum retry count for webhook job
        webhook_job_max_retry: 10
        # HTTP client timeout for webhook jobs (in seconds)
        webhook_job_http_client_timeout: 10
      
      chart:
        # Change the value of absolute_url to enabled can enable absolute url in chart
        absolute_url: disabled
      
      # Log configurations
      log:
        # options are debug, info, warning, error, fatal
        level: info
        # configs for logs in local storage
        local:
          # Log files are rotated log_rotate_count times before being removed. If count is 0, old versions are removed rather than rotated.
          rotate_count: 50
          # Log files are rotated only if they grow bigger than log_rotate_size bytes. If size is followed by k, the size is assumed to be in kilobytes.
          # If the M is used, the size is in megabytes, and if G is used, the size is in gigabytes. So size 100, size 100k, size 100M and size 100G
          # are all valid.
          rotate_size: 200M
          # The directory on your host that store log
          location: /data/harbor/logs
      
      #This attribute is for migrator to detect the version of the .cfg file, DO NOT MODIFY!
      _version: 2.11.0
      
      # Uncomment external_database if using external database.
      # external_database:
      #   harbor:
      #     host: harbor_db_host
      #     port: harbor_db_port
      #     db_name: harbor_db_name
      #     username: harbor_db_username
      #     password: harbor_db_password
      #     ssl_mode: disable
      #     max_idle_conns: 2
      #     max_open_conns: 0
      #   notary_signer:
      #     host: notary_signer_db_host
      #     port: notary_signer_db_port
      #     db_name: notary_signer_db_name
      #     username: notary_signer_db_username
      #     password: notary_signer_db_password
      #     ssl_mode: disable
      #   notary_server:
      #     host: notary_server_db_host
      #     port: notary_server_db_port
      #     db_name: notary_server_db_name
      #     username: notary_server_db_username
      #     password: notary_server_db_password
      #     ssl_mode: disable
      
      # Uncomment external_redis if using external Redis server
      # external_redis:
      #   # support redis, redis+sentinel
      #   # host for redis: <host_redis>:<port_redis>
      #   # host for redis+sentinel:
      #   #  <host_sentinel1>:<port_sentinel1>,<host_sentinel2>:<port_sentinel2>,<host_sentinel3>:<port_sentinel3>
      #   host: redis:6379
      #   password:
      #   # sentinel_master_set must be set to support redis+sentinel
      #   #sentinel_master_set:
      #   # db_index 0 is for core, it's unchangeable
      #   # db_index 1 is for jobservice
      #   # db_index 2 is for trivy
      #   db_index: 0
      
      # Uncomment uaa for trusting the certificate of uaa instance that is hosted via self-signed cert.
      # uaa:
      #   ca_file: /path/to/ca
      
      # Global proxy
      # Config http proxy for components, e.g. http://my.proxy.com:3128
      # Components doesn't need to connect to each others via http proxy.
      # Remove component from `components` array if want disable proxy
      # for it. If you want use proxy for replication, MUST enable proxy
      # for core and jobservice, and set `http_proxy` and `https_proxy`.
      # Add domain to the `no_proxy` list when want disable proxy
      # for some special registry
      proxy:
        http_proxy: ""
        https_proxy: ""
        no_proxy: ""
        components:
          - core
          - jobservice
          - trivy
      
      # metric:
      #   enabled: false
      #   port: 9090
      #   path: /metrics
      
      # Trace related config
      # only can enable one trace at the same time.
      # trace:
      #   enabled: true
      #   # set sample_rate to 1 if you wanna sampling 100% of trace data; set 0.5 if you wanna sampling 50% of trace data, and so forth
      #   sample_rate: 1
      #   # # namespace used to differentiate different harbor services
      #   # namespace:
      #   # # attributes is a key value dict contains user defined attributes used to initialize trace provider
      #   # attributes:
      #   #   application: harbor
      #   # # jaeger should be 1.26 or newer.
      #   # jaeger:
      #   #   endpoint: http://hostname:14268/api/traces
      #   #   username:
      #   #   password:
      #   #   agent_host: hostname
      #   #   agent_port: 6832
      #   # otel:
      #   #   endpoint: hostname:4318
      #   #   url_path: /v1/traces
      #   #   compression: false
      #   #   insecure: true
      #   #   timeout: 10s
      
      # enable purge _upload directories
      upload_purging:
        enabled: true
        # remove files in _upload directories which exist for a period of time, default is one week.
        age: 168h
        # the interval of the purge operations
        interval: 24h
        dryrun: false
      
      # cache layer configurations
      # If this feature enabled, harbor will cache the resource
      # `project/project_metadata/repository/artifact/manifest` in the redis
      # which can especially help to improve the performance of high concurrent
      # manifest pulling.
      # NOTICE
      # If you are deploying Harbor in HA mode, make sure that all the harbor
      # instances have the same behaviour, all with caching enabled or disabled,
      # otherwise it can lead to potential data inconsistency.
      cache:
        # default is not enabled.
        enabled: false
        # default keep cache for one day.
        expire_hours: 24

  - path: /usr/local/bin/debug-oidc
    permissions: '0755'
    content: |
      #!/bin/bash
      echo "=== OIDC Debug Information ==="
      echo ""
      
      echo "🔧 Harbor OIDC Configuration:"
      HARBOR_URL="http://utility-node.internal.lan"
      curl -s "$HARBOR_URL/api/v2.0/configurations" -u "admin:Harbor12345" | jq -r '
        {
          auth_mode: (.auth_mode.value // .auth_mode),
          oidc_name: (.oidc_name.value // .oidc_name),
          oidc_endpoint: (.oidc_endpoint.value // .oidc_endpoint),
          oidc_client_id: (.oidc_client_id.value // .oidc_client_id),
          oidc_scope: (.oidc_scope.value // .oidc_scope),
          oidc_verify_cert: (.oidc_verify_cert.value // .oidc_verify_cert),
          oidc_auto_onboard: (.oidc_auto_onboard.value // .oidc_auto_onboard),
          oidc_user_claim: (.oidc_user_claim.value // .oidc_user_claim),
          oidc_groups_claim: (.oidc_groups_claim.value // .oidc_groups_claim)
        }' 2>/dev/null || echo "Failed to get Harbor OIDC config"
      echo ""
      
      echo "🔧 Dex Container Status:"
      cd /opt/dex
      docker-compose ps
      echo ""
      
      echo "🔧 Dex Configuration Summary:"
      if [ -f /opt/dex/config.yaml ]; then
        echo "Issuer: $(grep '^issuer:' /opt/dex/config.yaml)"
        echo "GitHub Client ID: $(grep 'clientID:' /opt/dex/config.yaml | sed 's/.*clientID: *//')"
        echo "GitHub Redirect URI: $(grep 'redirectURI:' /opt/dex/config.yaml)"
        echo "Harbor Client ID: $(grep -A5 'staticClients:' /opt/dex/config.yaml | grep 'id:' | sed 's/.*id: *//')"
        echo "Harbor Redirect URIs: $(grep -A10 'staticClients:' /opt/dex/config.yaml | grep 'redirectURIs:' -A5)"
      else
        echo "Dex config file not found"
      fi
      echo ""
      
      echo "🔧 Testing Dex Endpoints:"
      echo "OIDC Discovery:"
      curl -s http://localhost:5556/.well-known/openid_configuration | jq . 2>/dev/null || echo "Failed to get OIDC discovery"
      echo ""
      
      echo "🔧 Recent Dex Logs (last 20 lines):"
      docker-compose logs --tail=20 dex
      echo ""
      
      echo "🔧 Recent Harbor Core Logs (last 10 lines):"
      cd /opt/harbor
      docker-compose logs --tail=10 core 2>/dev/null || echo "Harbor core logs not available"
      echo ""
      
      echo "🔧 Harbor System Info:"
      curl -s "$HARBOR_URL/api/v2.0/systeminfo" | jq -r '{
        harbor_version: .harbor_version,
        auth_mode: .auth_mode,
        oidc_provider_name: .oidc_provider_name
      }' 2>/dev/null || echo "Failed to get Harbor system info"
      echo ""
      
      echo "=== Debugging Commands ==="
      echo "Monitor Dex logs: cd /opt/dex && docker-compose logs -f"
      echo "Monitor Harbor logs: cd /opt/harbor && docker-compose logs -f core"
      echo "Test OIDC discovery: curl -s http://localhost:5556/.well-known/openid_configuration | jq ."
      echo "View this debug info: debug-oidc"

  - path: /usr/local/bin/harbor-manage
    permissions: '0755'
    content: |
      #!/bin/bash
      cd /opt/harbor
      case "$1" in
        start)
          docker-compose up -d
          ;;
        stop)
          docker-compose down
          ;;
        restart)
          docker-compose down
          sleep 5
          docker-compose up -d
          ;;
        status)
          docker-compose ps
          ;;
        logs)
          docker-compose logs -f
          ;;
        auth-status)
          echo "Harbor Authentication Status:"
          HARBOR_URL="http://utility-node.internal.lan"
          AUTH_MODE=$(curl -s "$HARBOR_URL/api/v2.0/configurations" -u "admin:Harbor12345" | jq -r '.auth_mode.value // .auth_mode' 2>/dev/null)
          if [ "$AUTH_MODE" = "oidc_auth" ]; then
            echo "✅ OIDC Authentication enabled"
            OIDC_NAME=$(curl -s "$HARBOR_URL/api/v2.0/configurations" -u "admin:Harbor12345" | jq -r '.oidc_name.value // .oidc_name' 2>/dev/null)
            OIDC_ENDPOINT=$(curl -s "$HARBOR_URL/api/v2.0/configurations" -u "admin:Harbor12345" | jq -r '.oidc_endpoint.value // .oidc_endpoint' 2>/dev/null)
            echo "   Provider: $OIDC_NAME"
            echo "   Endpoint: $OIDC_ENDPOINT"
          else
            echo "ℹ️  Database Authentication enabled (auth_mode: $AUTH_MODE)"
          fi
          ;;
        oidc-login-url)
          echo "GitHub OIDC Login URL: https://harbor.${RANCHER_DOMAIN}/c/oidc/login"
          ;;
        health)
          HARBOR_URL="http://utility-node.internal.lan"
          if curl -f -s "$HARBOR_URL/api/v2.0/systeminfo" >/dev/null; then
            echo "✅ Harbor API is healthy"
          else
            echo "❌ Harbor API is not responding"
          fi
          ;;
        jobservice-restart)
          echo "Restarting Harbor jobservice..."
          JOBSERVICE_CONTAINER=$(docker ps -a --filter "name=jobservice" --format "{{.Names}}" | head -1)
          if [ ! -z "$JOBSERVICE_CONTAINER" ]; then
            docker restart "$JOBSERVICE_CONTAINER"
            sleep 10
            docker ps --filter "name=jobservice"
          else
            echo "❌ Could not find jobservice container"
          fi
          ;;
        jobservice-logs)
          echo "Harbor jobservice logs:"
          JOBSERVICE_CONTAINER=$(docker ps -a --filter "name=jobservice" --format "{{.Names}}" | head -1)
          if [ ! -z "$JOBSERVICE_CONTAINER" ]; then
            docker logs -f "$JOBSERVICE_CONTAINER"
          else
            echo "❌ Could not find jobservice container"
          fi
          ;;
        *)
          echo "Usage: $0 {start|stop|restart|status|logs|auth-status|oidc-login-url|health|jobservice-restart|jobservice-logs}"
          exit 1
          ;;
      esac

  - path: /opt/dex/config.yaml.template
    permissions: '0644'
    content: |
      issuer: https://harbor-dex.${RANCHER_DOMAIN}
      
      storage:
        type: sqlite3
        config:
          file: /data/dex/dex.db
      
      web:
        http: 0.0.0.0:5556
        # tlsCert: /etc/dex/tls/tls.crt
        # tlsKey: /etc/dex/tls/tls.key
      
      connectors:
      - type: github
        id: github
        name: GitHub
        config:
          clientID: ${HARBOR_OIDC_CLIENT}
          clientSecret: ${HARBOR_OIDC_SECRET}
          redirectURI: https://harbor-dex.${RANCHER_DOMAIN}/callback
          orgs:
          - name: ${GITHUB_ORG}
            teams:
            - harbor_admins
            - harbor_users
          scopes:
          - read:org
          loadAllGroups: true
          teamNameField: slug
          useLoginAsID: false
      
      oauth2:
        skipApprovalScreen: true
        alwaysShowLoginScreen: false
      
      staticClients:
      - id: harbor
        redirectURIs:
        - 'https://harbor.${RANCHER_DOMAIN}/c/oidc/callback'
        name: 'Harbor Registry'
        secret: 'DEX_CLIENT_SECRET_PLACEHOLDER'
      
      enablePasswordDB: false
      
      logger:
        level: debug
        format: json

  - path: /opt/dex/docker-compose.yml.template
    permissions: '0644'
    content: |
      version: '3.8'
      
      services:
        dex:
          image: dexidp/dex:v2.30.0
          container_name: dex
          restart: unless-stopped
          ports:
            - "5556:5556"
          volumes:
            - /opt/dex/config.yaml:/etc/dex/config.yaml:ro
            - /data/dex:/data/dex
          # Run as root to avoid permission issues with volume mounts
          user: "0:0"
          command: ["dex", "serve", "/etc/dex/config.yaml"]
          networks:
            - dex-network
          healthcheck:
            test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:5556/healthz"]
            interval: 30s
            timeout: 10s
            retries: 3
            start_period: 40s
      
      networks:
        dex-network:
          driver: bridge

  - path: /usr/local/bin/test-oidc-flow
    permissions: '0755'
    content: |
      #!/bin/bash
      echo "=== Testing OIDC Flow ==="
      
      echo "1. Testing Dex OIDC Discovery..."
      DISCOVERY_URL="http://localhost:5556/.well-known/openid_configuration"
      DISCOVERY_RESULT=$(curl -s "$DISCOVERY_URL")
      if [ $? -eq 0 ] && echo "$DISCOVERY_RESULT" | jq . >/dev/null 2>&1; then
        echo "✅ Dex OIDC Discovery successful"
        echo "   Issuer: $(echo "$DISCOVERY_RESULT" | jq -r .issuer)"
        echo "   Auth endpoint: $(echo "$DISCOVERY_RESULT" | jq -r .authorization_endpoint)"
        echo "   Token endpoint: $(echo "$DISCOVERY_RESULT" | jq -r .token_endpoint)"
        echo "   Userinfo endpoint: $(echo "$DISCOVERY_RESULT" | jq -r .userinfo_endpoint)"
      else
        echo "❌ Dex OIDC Discovery failed"
        echo "   URL: $DISCOVERY_URL"
        echo "   Response: $DISCOVERY_RESULT"
      fi
      echo ""
      
      echo "2. Testing Harbor's ability to reach Dex..."
      # This tests from Harbor's perspective (through the external URL)
      HARBOR_TO_DEX_TEST=$(docker exec -it $(docker ps -q --filter "name=harbor-core") curl -s -w "%%{http_code}" "https://harbor-dex.${RANCHER_DOMAIN}/.well-known/openid-configuration" 2>/dev/null | tail -c 3)
      if [ "$HARBOR_TO_DEX_TEST" = "200" ]; then
        echo "✅ Harbor can reach Dex via external URL"
      else
        echo "❌ Harbor cannot reach Dex via external URL (HTTP $HARBOR_TO_DEX_TEST)"
        echo "   This could be a networking/DNS issue"
      fi
      echo ""
      
      echo "3. Checking Harbor OIDC Configuration..."
      HARBOR_URL="http://utility-node.internal.lan"
      HARBOR_CONFIG=$(curl -s "$HARBOR_URL/api/v2.0/configurations" -u "admin:Harbor12345")
      AUTH_MODE=$(echo "$HARBOR_CONFIG" | jq -r '.auth_mode.value // .auth_mode' 2>/dev/null)
      OIDC_ENDPOINT=$(echo "$HARBOR_CONFIG" | jq -r '.oidc_endpoint.value // .oidc_endpoint' 2>/dev/null)
      
      if [ "$AUTH_MODE" = "oidc_auth" ]; then
        echo "✅ Harbor is configured for OIDC authentication"
        echo "   OIDC Provider: $(echo "$HARBOR_CONFIG" | jq -r '.oidc_name.value // .oidc_name' 2>/dev/null)"
        echo "   OIDC Endpoint: $OIDC_ENDPOINT"
        echo "   Client ID: $(echo "$HARBOR_CONFIG" | jq -r '.oidc_client_id.value // .oidc_client_id' 2>/dev/null)"
      else
        echo "❌ Harbor is not configured for OIDC (current mode: $AUTH_MODE)"
      fi
      echo ""
      
      echo "4. Testing the complete OIDC login URL..."
      LOGIN_URL="https://harbor.${RANCHER_DOMAIN}/c/oidc/login"
      LOGIN_RESPONSE=$(curl -s -w "%%{http_code}" -o /dev/null "$LOGIN_URL" 2>/dev/null)
      if [ "$LOGIN_RESPONSE" = "302" ] || [ "$LOGIN_RESPONSE" = "200" ]; then
        echo "✅ Harbor OIDC login endpoint accessible (HTTP $LOGIN_RESPONSE)"
        echo "   URL: $LOGIN_URL"
      else
        echo "❌ Harbor OIDC login endpoint issue (HTTP $LOGIN_RESPONSE)"
        echo "   URL: $LOGIN_URL"
      fi
      echo ""
      
      echo "=== Common Issues and Solutions ==="
      echo "• If Dex discovery fails: Check if Dex container is running"
      echo "• If Harbor can't reach Dex: Check DNS/networking from Harbor to Dex"
      echo "• If OIDC login fails: Check Dex logs for authorization errors"
      echo "• If 'internal server error': Usually a configuration mismatch"
      echo ""
      echo "=== Useful Commands ==="
      echo "• Monitor Dex logs: cd /opt/dex && docker-compose logs -f"
      echo "• Monitor Harbor logs: cd /opt/harbor && docker-compose logs -f core"
      echo "• Debug OIDC config: debug-oidc"
      echo "• Run this test again: test-oidc-flow"

  - path: /usr/local/bin/performance-tuning.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      echo "🔧 Applying performance tuning..."
      
      # CPU Governor
      GOVERNOR_PATH="/sys/devices/system/cpu/cpu0/cpufreq/scaling_governor"
      if [ -f "$GOVERNOR_PATH" ]; then
        for CPU in /sys/devices/system/cpu/cpu[0-9]*; do
          echo performance > "$CPU/cpufreq/scaling_governor" 2>/dev/null || true
        done
        echo "✅ CPU governors set to performance"
      else
        echo "⚠️ CPU frequency scaling not available — skipping governor tuning"
      fi
      
      # Scheduler tuning for SSDs
      for dev in /sys/block/sd*/queue/scheduler; do
        echo mq-deadline > "$dev"
      done

      systemctl disable apt-daily.timer
      systemctl disable snapd
      systemctl disable motd-news.timer
      
      # Sysctl parameters
      cat <<EOF > /etc/sysctl.d/99-performance-tuning.conf
      fs.inotify.max_user_watches=524288
      vm.max_map_count=262144
      net.core.rmem_max=134217728
      net.core.wmem_max=134217728
      net.ipv4.tcp_rmem=4096 87380 134217728
      net.ipv4.tcp_wmem=4096 65536 134217728
      net.ipv4.tcp_congestion_control=bbr
      vm.swappiness=10
      vm.dirty_ratio=15
      vm.dirty_background_ratio=5
      EOF
      
      sysctl --system

      # 2) Pick all “large-MTU” interfaces automatically
      #    (adjust the grep if your jumbo-MTU devices have a different naming scheme)
      for IF in $(ip -o link show | awk -F': ' '{print $2}' | grep -E '^(eth|vtnet)'); do
        # bring MTU up before offloads
        ip link set dev $IF mtu 9000

        # enable big-packet offloads
        ethtool -K $IF gro on gso on tso on rx on tx on

        # increase the kernel TX queue length
        ip link set dev $IF txqueuelen 10000
      done

      # 3) Sysctls — bump backlog and buffers for all interfaces
      #    and enable TCP MTU probing (helps path-MTU discovery)
      sysctl -w net.core.netdev_max_backlog=250000
      sysctl -w net.core.rmem_max=134217728
      sysctl -w net.core.wmem_max=134217728
      sysctl -w net.ipv4.tcp_rmem="4096 87380 134217728"
      sysctl -w net.ipv4.tcp_wmem="4096 87380 134217728"
      sysctl -w net.ipv4.tcp_mtu_probing=1

      # 4) Persist sysctls
      cat <<EOF >/etc/sysctl.d/99-network-opt.conf
      net.core.netdev_max_backlog=250000
      net.core.rmem_max=134217728
      net.core.wmem_max=134217728
      net.ipv4.tcp_rmem=4096 87380 134217728
      net.ipv4.tcp_wmem=4096 87380 134217728
      net.ipv4.tcp_mtu_probing=1
      EOF

      # 5) Persist ethtool + txqueuelen via a systemd service
      cat <<'EOF' >/etc/systemd/system/network-tweaks.service
      [Unit]
      Description=Apply ethtool offloads + txqueuelen
      After=network-online.target
      Wants=network-online.target

      [Service]
      Type=oneshot
      ExecStart=/usr/local/bin/network-tweaks.sh

      [Install]
      WantedBy=multi-user.target
      EOF

      # Install the same script to /usr/local/bin and make it executable
            cat <<'EOF' >/usr/local/bin/network-tweaks.sh
      #!/bin/bash
      set -e
      for IF in $(ip -o link show | awk -F': ' '{print $2}' | grep -E '^(eth|vtnet)'); do
        CURRENT_MTU=$(ip -o link show "$IF" | awk '{print $4}')
        if [ "$CURRENT_MTU" != "9000" ]; then
          ip link set dev "$IF" mtu 9000
        fi
        ethtool -K $IF gro on gso on tso on rx on tx on
        ip link set dev $IF txqueuelen 10000
      done
      EOF

      chmod +x /usr/local/bin/network-tweaks.sh

      systemctl daemon-reload
      systemctl enable network-tweaks

      echo "✅ Network tweaks applied and will persist across reboots."
      
      # Limit nofile
      cat <<EOF > /etc/security/limits.d/99-nofile.conf
      * soft nofile 1048576
      * hard nofile 1048576
      EOF
      
      # Systemd service overrides
      mkdir -p /etc/systemd/system/containerd.service.d
      mkdir -p /etc/systemd/system/rke2-server.service.d
      mkdir -p /etc/systemd/system/rke2-agent.service.d
      
      cat <<EOF > /etc/systemd/system/containerd.service.d/override.conf
      [Service]
      LimitNOFILE=1048576
      LimitNPROC=65536
      TasksMax=infinity
      EOF
      
      cat <<EOF > /etc/systemd/system/rke2-server.service.d/override.conf
      [Service]
      LimitNOFILE=1048576
      LimitNPROC=65536
      TasksMax=infinity
      EOF
      
      cat <<EOF > /etc/systemd/system/rke2-agent.service.d/override.conf
      [Service]
      LimitNOFILE=1048576
      LimitNPROC=65536
      TasksMax=infinity
      EOF

      # cat <<EOF | sudo tee /etc/profile.d/http_proxy.sh
      # export HTTP_PROXY="http://10.0.0.10:3128"
      # export HTTPS_PROXY="http://10.0.0.10:3128"
      # export NO_PROXY="127.0.0.1,localhost,10.0.0.0/24,10.96.0.0/12,192.168.0.0/16"
      # EOF

      # sudo chmod +x /etc/profile.d/http_proxy.sh

      # Unbind kernel workqueues from specific CPUs (recommended on NUMA/multicore systems)
      if [ -e /sys/module/workqueue/parameters/disable_bound ]; then
        echo y > /sys/module/workqueue/parameters/disable_bound
        echo "✅ Enabled unbound kernel workqueues for better scalability"
      fi

      # 6) Add GRUB performance kernel parameters
      echo "✅ Adding performance kernel parameters to GRUB..."
      GRUB_CONFIG_FILE="/etc/default/grub"

      # Append only if not already present
      if ! grep -q "workqueue.disable_bound=1" "$GRUB_CONFIG_FILE"; then
        sed -i 's/GRUB_CMDLINE_LINUX_DEFAULT="/GRUB_CMDLINE_LINUX_DEFAULT="workqueue.disable_bound=1 intel_pstate=disable processor.max_cstate=1 idle=poll transparent_hugepage=never mitigations=off /' "$GRUB_CONFIG_FILE"
        update-grub
        echo "✅ GRUB updated. Kernel parameters will apply on next boot."
      else
        echo "⚠️ GRUB already configured. Skipping update."
      fi
      
      systemctl daemon-reload

  - path: /usr/local/bin/setup-hosts-dns.sh
    permissions: '0755'
    content: |
      #!/bin/bash

      # Add hostname to /etc/hosts without modifying network config
      HOSTNAME=$(hostname)
      grep -q "127.0.1.1 $HOSTNAME" /etc/hosts || echo "127.0.1.1 $HOSTNAME" >> /etc/hosts

      # ---------------------------------------------
      # 🧹 Disable systemd-resolved to free port 53
      # ---------------------------------------------
      echo "🧹 Disabling systemd-resolved..."
      sudo systemctl stop systemd-resolved || true
      sudo systemctl disable systemd-resolved || true
      sudo rm -f /etc/resolv.conf
      echo "nameserver 127.0.0.1" | sudo tee /etc/resolv.conf > /dev/null

      # ---------------------------------------------
      # ⚙️ Configure dnsmasq split-DNS
      # ---------------------------------------------
      sudo tee /etc/dnsmasq.d/k8s-split-dns.conf > /dev/null <<EOF
      # Forward Kubernetes service lookups to CoreDNS
      server=/svc.cluster.local/10.43.0.10

      # Forward internal LAN lookups to LAN DNS
      server=/internal.lan/10.0.0.10

      # Forward everything else to LAN DNS
      server=10.0.0.10

      # Listen locally
      listen-address=127.0.0.1

      # Don't use /etc/resolv.conf
      no-resolv
      EOF

      # Ensure dnsmasq global config allows localhost
      sudo sed -i 's/^#\?listen-address=.*/listen-address=127.0.0.1/' /etc/dnsmasq.conf || echo 'listen-address=127.0.0.1' | sudo tee -a /etc/dnsmasq.conf

      sudo systemctl enable dnsmasq
      sudo systemctl restart dnsmasq

      # Confirm DNS server
      echo "✅ dnsmasq is resolving:"
      echo "  - *.svc.cluster.local → 10.43.0.10"
      echo "  - *.internal.lan      → 10.0.0.10"
      echo "  - all others          → 10.0.0.10"

      # ---------------------------------------------
      # 🛡 IPv6 Hard Disable (Optional)
      # ---------------------------------------------
      sudo modprobe -r ip6_tables || true
      sudo modprobe -r nf_defrag_ipv6 || true
      sudo modprobe -r nf_conntrack_ipv6 || true
      sudo modprobe -r ipv6 || true

      echo "install ipv6 /bin/true" | sudo tee /etc/modprobe.d/force-disable-ipv6.conf
      echo "install ip6_tables /bin/true" | sudo tee -a /etc/modprobe.d/force-disable-ipv6.conf

      if [ -e /usr/sbin/ip6tables ]; then
        sudo mv /usr/sbin/ip6tables /usr/sbin/ip6tables.disabled
      fi

      sudo tee /etc/modprobe.d/disable-ipv6.conf > /dev/null <<EOF
      blacklist ipv6
      blacklist nf_conntrack_ipv6
      blacklist nf_defrag_ipv6
      blacklist ip6_tables
      EOF

      sudo update-initramfs -u

      # ---------------------------------------------
      # 🔁 Force sysctl re-apply on boot
      # ---------------------------------------------
      sudo tee /etc/systemd/system/sysctl-ensure.service > /dev/null <<EOF
      [Unit]
      Description=Force re-apply sysctl settings
      After=network.target
      Wants=network.target

      [Service]
      Type=oneshot
      ExecStart=/sbin/sysctl --system

      [Install]
      WantedBy=multi-user.target
      EOF

      sudo systemctl enable sysctl-ensure.service

  - path: /etc/systemd/system/setup-hosts-dns.service
    permissions: '0644'
    content: |
      [Unit]
      Description=Setup hosts and DNS configuration
      After=network-online.target
      Wants=network-online.target

      [Service]
      Type=oneshot
      ExecStart=/usr/local/bin/setup-hosts-dns.sh
      RemainAfterExit=true

      [Install]
      WantedBy=multi-user.target

  - path: /etc/sysctl.d/99-disable-ipv6.conf
    permissions: '0644'
    content: |
      net.ipv6.conf.all.disable_ipv6 = 1
      net.ipv6.conf.default.disable_ipv6 = 1
      net.ipv6.conf.lo.disable_ipv6 = 1

  - path: /etc/default/grub.d/99-disable-ipv6.cfg
    permissions: '0644'
    content: |
      GRUB_CMDLINE_LINUX="$GRUB_CMDLINE_LINUX ipv6.disable=1"

  - path: /usr/local/bin/disable-swap.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      swapoff -a
      cp /etc/fstab /etc/fstab.bak
      sed -i '/^[^#].*\bswap\b/s/^/#/' /etc/fstab
      echo fs.inotify.max_user_watches=524288 | sudo tee /etc/sysctl.d/99-inotify.conf

  - path: /usr/local/bin/setup-docker.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      set -e  # Exit on any error
      exec > >(tee -a /var/log/setup-docker.log) 2>&1  # Log all output
      
      echo "$(date): Setting up Docker..."
      
      # Retry mechanism for downloading Docker GPG key
      for i in {1..5}; do
        if curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -; then
          echo "✅ Docker GPG key added successfully"
          break
        else
          echo "⚠️  Attempt $i failed, retrying in 10 seconds..."
          sleep 10
        fi
      done
      
      sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
      sudo apt-get update -y
      DEBIAN_FRONTEND=noninteractive sudo apt-get install -y docker-ce docker-ce-cli containerd.io

      # Install docker-compose immediately after Docker
      echo "$(date): 📦 Installing docker-compose..."
      curl -L "https://github.com/docker/compose/releases/download/v2.24.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
      chmod +x /usr/local/bin/docker-compose
      
      # Verify docker-compose installation
      docker-compose --version
      echo "✅ Docker and docker-compose installed successfully"

      # sudo mkdir -p /etc/systemd/system/docker.service.d
      # cat <<EOF | sudo tee /etc/systemd/system/docker.service.d/10-http-proxy.conf
      # [Service]
      # Environment="HTTP_PROXY=http://10.0.0.10:3128" "HTTPS_PROXY=http://10.0.0.10:3128" "NO_PROXY=127.0.0.1,localhost,10.0.0.0/24,10.96.0.0/12"
      # EOF

      # sudo mkdir -p /etc/systemd/system/containerd.service.d
      # cat <<EOF | sudo tee /etc/systemd/system/containerd.service.d/10-http-proxy.conf
      # [Service]
      # Environment="HTTP_PROXY=http://10.0.0.10:3128" "HTTPS_PROXY=http://10.0.0.10:3128" "NO_PROXY=127.0.0.1,localhost,10.0.0.0/24,10.96.0.0/12"
      # EOF

  - path: /etc/security/limits.d/99-nofile.conf
    permissions: '0644'
    content: |
      * soft nofile 1048576
      * hard nofile 1048576

  - path: /etc/systemd/system/containerd.service.d/override.conf
    permissions: '0644'
    content: |
      [Service]
      LimitNOFILE=1048576
      LimitNPROC=65536
      TasksMax=infinity

  - path: /etc/systemd/system/rke2-server.service.d/override.conf
    permissions: '0644'
    content: |
      [Service]
      LimitNOFILE=1048576
      LimitNPROC=65536
      TasksMax=infinity

  - path: /etc/systemd/system/rke2-agent.service.d/override.conf
    permissions: '0644'
    content: |
      [Service]
      LimitNOFILE=1048576
      LimitNPROC=65536
      TasksMax=infinity

  - path: /usr/local/bin/setup-fluentbit.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      set -e
      echo "📦 Configuring Fluent Bit for GCP logging..."

      mkdir -p /etc/google-cloud-ops-agent
      echo "${GCP_LOGGING_KEY}" | base64 -d > /etc/google-cloud-ops-agent/logging-key.json

      mkdir -p /usr/share/keyrings
      KEYRING_PATH="/usr/share/keyrings/fluent-bit-archive-keyring.gpg"

      # Clean up any broken or partial key
      rm -f "$KEYRING_PATH"

      until curl -fsSL https://packages.fluentbit.io/fluentbit.key | gpg --dearmor --no-tty --batch -o "$KEYRING_PATH"; do
        echo "Waiting for DNS resolution or valid key for fluentbit.key..."
        sleep 5
        rm -f "$KEYRING_PATH"  # ensure no corrupted key is left behind
      done

      DISTRO=$(lsb_release -cs)
      if [ "$DISTRO" = "${UBUNTU_RELEASE_CODE_NAME}" ]; then
        echo "Detected codename $DISTRO unsupported, falling back to jammy"
        DISTRO="jammy"
      fi

      echo "deb [signed-by=/usr/share/keyrings/fluent-bit-archive-keyring.gpg] https://packages.fluentbit.io/ubuntu/$DISTRO $DISTRO main" > /etc/apt/sources.list.d/fluent-bit.list
      sudo apt-get update
      sudo apt-get install -y td-agent-bit

      mkdir -p /etc/td-agent-bit/conf.d

      cat <<EOF > /etc/td-agent-bit/td-agent-bit.conf
      [SERVICE]
          flush         1
          daemon        Off
          log_level     info
          plugins_file  plugins.conf
          parsers_file  parsers.conf
          http_server   Off
          http_listen   0.0.0.0
          http_port     2020
          storage.metrics on

      @INCLUDE /etc/td-agent-bit/conf.d/*.conf
      EOF

      # cloud-init logs
      cat <<EOF > /etc/td-agent-bit/conf.d/cloudinit.conf
      [INPUT]
          Name               tail
          Mem_Buf_Limit     10MB
          Path               /var/log/cloud-init*.log
          Tag                cloudinit
          Read_from_Head     Off
          DB                 /var/log/flb_cloudinit.db
          DB.Sync            normal
          Skip_Long_Lines    On

      [FILTER]
          Name      parser
          Match     cloudinit
          Key_Name  log
          Parser    cloudinit_parser
          Reserve_Data true
          Preserve_Key true

      [FILTER]
          Name modify
          Match cloudinit
          Add logName cloudinit

      [FILTER]
          Name    nest
          Match   cloudinit
          Operation nest
          Wildcard process
          Wildcard pid
          Wildcard timestamp
          Nested_under logging.googleapis.com/labels

      [OUTPUT]
          Name  stackdriver
          Match cloudinit
          google_service_credentials /etc/google-cloud-ops-agent/logging-key.json
          resource ${MONITORED_RESOURCE_TYPE}
          location ${MONITORED_RESOURCE_LOCATION}
          namespace ${MONITORED_RESOURCE_NAMESPACE}
          node_id ${MONITORED_RESOURCE_NODE_ID}
          log_name_key cloudinit
      EOF

      # journal logs (systemd, rke2, etc.)
      cat <<EOF > /etc/td-agent-bit/conf.d/journald.conf
      [INPUT]
          Name    systemd
          Tag     journal.*
          Mem_Buf_Limit     10MB
          DB      /var/log/flb_journal.db
          Systemd_Filter  _SYSTEMD_UNIT=bootstrap-rke2.service
          Systemd_Filter  _SYSTEMD_UNIT=rke2-server.service
          Systemd_Filter  _SYSTEMD_UNIT=cloud-final.service
          Read_From_Tail  On

      [FILTER]
          Name      parser
          Match     journal.*
          Key_Name  MESSAGE
          Parser    journald_parser
          Reserve_Data true
          Preserve_Key true

      [FILTER]
          Name modify
          Match journal.*
          Add logName systemd-journal

      [FILTER]
          Name   modify
          Match  journal.*
          Rename message textPayload

      [FILTER]
          Name    nest
          Match   journal.*
          Operation nest
          Wildcard identifier
          Wildcard pid
          Wildcard hostname
          Nested_under logging.googleapis.com/labels

      [OUTPUT]
          Name  stackdriver
          Match journal.*
          google_service_credentials /etc/google-cloud-ops-agent/logging-key.json
          resource ${MONITORED_RESOURCE_TYPE}
          location ${MONITORED_RESOURCE_LOCATION}
          namespace ${MONITORED_RESOURCE_NAMESPACE}
          node_id ${MONITORED_RESOURCE_NODE_ID}
          log_name_key systemd-journal
      EOF

      # Optional: catch all syslog (if rsyslog is enabled)
      cat <<EOF > /etc/td-agent-bit/conf.d/syslog.conf
      [INPUT]
          Name              tail
          Mem_Buf_Limit     10MB
          Path              /var/log/syslog
          Tag               syslog
          Read_from_Head    Off
          DB                /var/log/flb_syslog.db
          Skip_Long_Lines   On

      [FILTER]
          Name   parser
          Match  syslog
          Key_Name log
          Parser syslog_parser
          Reserve_Data true
          Preserve_Key true

      [FILTER]
          Name   modify
          Match  syslog
          Rename message textPayload

      [FILTER]
          Name modify
          Match syslog
          Add logName syslog

      [FILTER]
          Name    nest
          Match   syslog
          Operation nest
          Wildcard hostname
          Wildcard identifier
          Wildcard pid
          Nested_under logging.googleapis.com/labels

      [OUTPUT]
          Name  stackdriver
          Match syslog
          google_service_credentials /etc/google-cloud-ops-agent/logging-key.json
          resource ${MONITORED_RESOURCE_TYPE}
          location ${MONITORED_RESOURCE_LOCATION}
          namespace ${MONITORED_RESOURCE_NAMESPACE}
          node_id ${MONITORED_RESOURCE_NODE_ID}
          log_name_key syslog
      EOF

      # cloudinit_parser
      cat <<EOF >> /etc/td-agent-bit/parsers.conf

      [PARSER]
          Name        cloudinit_parser
          Format      regex
          Regex       ^(?<timestamp>[^ ]+\s[^ ]+) (?<process>[^\[]+)\[(?<pid>\d+)\]:(?<message>.*)
          Time_Key    timestamp
          Time_Format %Y-%m-%d %H:%M:%S

      [PARSER]
          Name        syslog_parser
          Format      regex
          Regex       ^(?<timestamp>\w{3} +\d{1,2} \d{2}:\d{2}:\d{2}) (?<hostname>[^\s]+) (?<identifier>[^:]+): (?<message>.*)
          Time_Key    timestamp
          Time_Format %b %d %H:%M:%S

      [PARSER]
          Name        journald_parser
          Format      regex
          Regex       ^(?<timestamp>\d{4}-\d{2}-\d{2}T[0-9:.+\-Z]+) (?<hostname>[^\s]+) (?<identifier>[^\[]+)\[(?<pid>\d+)\]: (?<message>.*)
          Time_Key    timestamp
          Time_Format %Y-%m-%dT%H:%M:%S.%L%z
      EOF

      systemctl daemon-reload
      systemctl enable td-agent-bit
      systemctl restart td-agent-bit

  - path: /usr/local/bin/debug-cloud-init.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      echo "=== Cloud-Init Debug Information ==="
      echo
      echo "Cloud-Init Status:"
      cloud-init status --long
      echo
      echo "Cloud-Init Logs (last 50 lines):"
      tail -n 50 /var/log/cloud-init.log
      echo
      echo "Cloud-Init Output Logs (last 50 lines):"
      tail -n 50 /var/log/cloud-init-output.log
      echo
      echo "Initial Setup Log:"
      if [ -f /var/log/initial-setup.log ]; then
        tail -n 50 /var/log/initial-setup.log
      else
        echo "No initial setup log found"
      fi
      echo
      echo "Setup Utility Log:"
      if [ -f /var/log/setup-utility.log ]; then
        tail -n 50 /var/log/setup-utility.log
      else
        echo "No setup utility log found"
      fi
      echo
      echo "Setup Docker Log:"
      if [ -f /var/log/setup-docker.log ]; then
        tail -n 50 /var/log/setup-docker.log
      else
        echo "No setup docker log found"
      fi
      echo
      echo "SSH Key Status:"
      echo "Ubuntu user SSH keys:"
      cat /home/ubuntu/.ssh/authorized_keys 2>/dev/null || echo "No SSH keys found"
      echo
      echo "System Services Status:"
      systemctl status initial-setup.service
      echo
      echo "Docker Status:"
      systemctl status docker 2>/dev/null || echo "Docker not installed/running"
      echo
      echo "Harbor Status:"
      harbor-manage status 2>/dev/null || echo "Harbor not available"

runcmd:
  - systemctl daemon-reload
  - systemctl enable initial-setup.service
  - systemctl start initial-setup.service